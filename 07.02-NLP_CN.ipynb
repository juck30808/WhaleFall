{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP - the Youtube Comment (CN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "wb = Workbook()\n",
    "wb.save(\"input/NLP-save1.xlsx\")  #Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>iD</th>\n",
       "      <th>tim</th>\n",
       "      <th>nam</th>\n",
       "      <th>aythimg</th>\n",
       "      <th>mess1</th>\n",
       "      <th>type</th>\n",
       "      <th>channel</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>The Newest Comment output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3941548</td>\n",
       "      <td>2022-05-13 14:59:58</td>\n",
       "      <td>Mr. J</td>\n",
       "      <td>https://yt4.ggpht.com/ytc/AKedOLTJ4kuN86d80H6C...</td>\n",
       "      <td>政治防疫 糙</td>\n",
       "      <td>textMessage</td>\n",
       "      <td>ctitv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3941547</td>\n",
       "      <td>2022-05-13 14:59:57</td>\n",
       "      <td>閑聊</td>\n",
       "      <td>https://yt4.ggpht.com/ytc/AKedOLR1B9ZW9uDWo-oG...</td>\n",
       "      <td>3+4及與病毒共存害死那麼多人？死亡人數還不夠多嗎？</td>\n",
       "      <td>textMessage</td>\n",
       "      <td>ctitv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3941546</td>\n",
       "      <td>2022-05-13 14:59:55</td>\n",
       "      <td>Sausan Shunnar</td>\n",
       "      <td>https://yt4.ggpht.com/ytc/AKedOLQbB5RL14GTzuXe...</td>\n",
       "      <td>為什麼報應不在政腐 反而在人民身上 為什麼老百姓要丟性命買單</td>\n",
       "      <td>textMessage</td>\n",
       "      <td>ctitv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3941545</td>\n",
       "      <td>2022-05-13 14:59:54</td>\n",
       "      <td>Heshirley蕃秀</td>\n",
       "      <td>https://yt4.ggpht.com/ytc/AKedOLSCoj6fA5UmDI8Q...</td>\n",
       "      <td>原來政策也可以殺死人</td>\n",
       "      <td>textMessage</td>\n",
       "      <td>ctitv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3941543</td>\n",
       "      <td>2022-05-13 14:59:52</td>\n",
       "      <td>熊男くまお</td>\n",
       "      <td>https://yt4.ggpht.com/ytc/AKedOLREjsH_39Sh1UIt...</td>\n",
       "      <td>皇明祖訓 {'id': '', 'txt': '', 'url': 'https://www...</td>\n",
       "      <td>textMessage</td>\n",
       "      <td>ctitv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       iD                  tim             nam  \\\n",
       "0           0  3941548  2022-05-13 14:59:58           Mr. J   \n",
       "1           1  3941547  2022-05-13 14:59:57              閑聊   \n",
       "2           2  3941546  2022-05-13 14:59:55  Sausan Shunnar   \n",
       "3           3  3941545  2022-05-13 14:59:54     Heshirley蕃秀   \n",
       "4           4  3941543  2022-05-13 14:59:52           熊男くまお   \n",
       "\n",
       "                                             aythimg  \\\n",
       "0  https://yt4.ggpht.com/ytc/AKedOLTJ4kuN86d80H6C...   \n",
       "1  https://yt4.ggpht.com/ytc/AKedOLR1B9ZW9uDWo-oG...   \n",
       "2  https://yt4.ggpht.com/ytc/AKedOLQbB5RL14GTzuXe...   \n",
       "3  https://yt4.ggpht.com/ytc/AKedOLSCoj6fA5UmDI8Q...   \n",
       "4  https://yt4.ggpht.com/ytc/AKedOLREjsH_39Sh1UIt...   \n",
       "\n",
       "                                               mess1         type channel  \\\n",
       "0                                             政治防疫 糙  textMessage   ctitv   \n",
       "1                         3+4及與病毒共存害死那麼多人？死亡人數還不夠多嗎？  textMessage   ctitv   \n",
       "2                     為什麼報應不在政腐 反而在人民身上 為什麼老百姓要丟性命買單  textMessage   ctitv   \n",
       "3                                         原來政策也可以殺死人  textMessage   ctitv   \n",
       "4  皇明祖訓 {'id': '', 'txt': '', 'url': 'https://www...  textMessage   ctitv   \n",
       "\n",
       "   Unnamed: 8  The Newest Comment output  \n",
       "0         NaN                       14.0  \n",
       "1         NaN                        NaN  \n",
       "2         NaN                        NaN  \n",
       "3         NaN                        NaN  \n",
       "4         NaN                        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('input/NLP-excel.xlsx')\n",
    "data.head()\n",
    "#type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                  政治防疫 糙\n",
       "1                              3+4及與病毒共存害死那麼多人？死亡人數還不夠多嗎？\n",
       "2                          為什麼報應不在政腐 反而在人民身上 為什麼老百姓要丟性命買單\n",
       "3                                              原來政策也可以殺死人\n",
       "4       皇明祖訓 {'id': '', 'txt': '', 'url': 'https://www...\n",
       "                              ...                        \n",
       "2080    茗淨檔以炸片為生{'id': '', 'txt': ':beaming_face_with_...\n",
       "2081                              中天 在YouTube 真的丟臉 應該在微博報\n",
       "2082                                林婉儿 小粉紅封城還是喜歡翻牆來看台灣~~\n",
       "2083                   说全了，应该是今天可筛数字是不是破7W。快筛不够，实际数据没办法获知\n",
       "2084                      1450多來 衝流量喔，安靜 學習，學海 無崖，少害人了。。。\n",
       "Name: mess1, Length: 2085, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['mess1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Titles']\n"
     ]
    }
   ],
   "source": [
    "## Excel 把檔案轉換成 dataFrame 轉換成 excel (資料庫匯入用)\n",
    "import datetime\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.cell.cell import WriteOnlyCell\n",
    "\n",
    "wb = Workbook()\n",
    "ws1 = wb.active\n",
    "ws1.title = \"Titles\"\n",
    "for r in dataframe_to_rows(data, index=True, header=True):\n",
    "    ws1.append(r)\n",
    "\n",
    "ws1[\"K1\"] = \"TheOutput\"\n",
    "ws1[\"K2\"] = str(datetime.date.today())\n",
    "\n",
    "wb.save(\"input/NLP-save1.xlsx\")\n",
    "print(wb.sheetnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Words Count : Characters, and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      6\n",
       "1     26\n",
       "2     30\n",
       "3     10\n",
       "4    218\n",
       "Name: char_count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the char len\n",
    "data['char_count'] = data['mess1'].apply(len) \n",
    "data['char_count'].head()\n",
    "\n",
    "# 0     6  政治防疫 糙\n",
    "# 1    26  3+4及與病毒共存害死那麼多人？死亡人數還不夠多嗎？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2\n",
       "1     1\n",
       "2     3\n",
       "3     1\n",
       "4    12\n",
       "Name: word_count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the Sentence Len\n",
    "data['word_count'] = data['mess1'].apply(lambda x: len(x.split()))\n",
    "data['word_count'].head() # Word Count(段落)\n",
    "\n",
    "# 0     2  政治防疫 糙\n",
    "# 1     1  3+4及與病毒共存害死那麼多人？死亡人數還不夠多嗎？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2.000000\n",
       "1    13.000000\n",
       "2     7.500000\n",
       "3     5.000000\n",
       "4    16.769231\n",
       "Name: word_density, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the Word Density\n",
    "data['word_density'] = data['char_count'] / (data['word_count']+1)\n",
    "data['word_density'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     1\n",
       "2     0\n",
       "3     0\n",
       "4    66\n",
       "Name: punctuation_count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the Punctuation Count(, . or and special Punctuation)\n",
    "import string\n",
    "\n",
    "punctuation = string.punctuation\n",
    "data['punctuation_count'] = data['mess1'].apply(\n",
    "    lambda x:len(\"\".join(_ for _ in x if _ in punctuation))) \n",
    "data['punctuation_count'].head()\n",
    "\n",
    "# 0     0  政治防疫 糙\n",
    "# 1     1  3+4及與病毒共存害死那麼多人？死亡人數還不夠多嗎？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Removing Characters\n",
    "\n",
    ">Accented characters are important elements which are used to signify emphasis on a particular word during pronunciation or understanding. In some instances, the accent mark also clarifies the meaning of a word, which might be different without the accent. While their use in English is largely limited but there are very good chances that you will come across accented characters/letters in a free text corpus. \n",
    "\n",
    "重音字符是重要的元素，用於在發音或理解過程中表示對特定單詞的強調。 在某些情況下，重音標記還可以闡明單詞的含義，如果沒有重音，它可能會有所不同。 雖然它們在英語中的使用在很大程度上受到限制，但您很有可能會在自由文本語料庫中遇到重音字符/字母：Word as résumé, café, prótest, divorcé, coördinate, exposé, latté etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencc-python-reimplemented in c:\\users\\cti110016\\anaconda3\\lib\\site-packages (0.1.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencc-python-reimplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'opencc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CTI110~1\\AppData\\Local\\Temp/ipykernel_22084/468084245.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## 函數方式刪除 Def\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mopencc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOpenCC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdrop_numbers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m#Drop num\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlist_text_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'opencc'"
     ]
    }
   ],
   "source": [
    "## 函數方式刪除 Def\n",
    "from opencc import OpenCC\n",
    "\n",
    "def drop_numbers(list_text):   #Drop num\n",
    "    list_text_new = []\n",
    "    for i in list_text:\n",
    "        if not re.search('\\d', i):\n",
    "            list_text_new.append(i)\n",
    "    return ''.join(list_text_new)\n",
    "\n",
    "def drop_url(list_text):       #drop_youtube_url{}\n",
    "    return re.sub(r\"\\{.+\\}\",\"\", list_text)\n",
    "\n",
    "def drop_en(list_text):        #drop_english\n",
    "    return re.sub(r\"[a-zA-Z0-9@+.]\",\"\", list_text)\n",
    "\n",
    "def translate(list_text):\n",
    "    cc = OpenCC('s2t')\n",
    "    return cc.convert(list_text)\n",
    "\n",
    "def regex_change(line):        #reference\n",
    "    username_regex = re.compile(r\"^\\d+::\")\n",
    "    url_regex = re.compile(r\"\"\"\n",
    "        (https?://)?\n",
    "        ([a-zA-Z0-9]+)\n",
    "        (\\.[a-zA-Z0-9]+)\n",
    "        (\\.[a-zA-Z0-9]+)*\n",
    "        (/[a-zA-Z0-9]+)*\"\"\", re.VERBOSE|re.IGNORECASE)\n",
    "    data_regex = re.compile(u\"\"\"年 |月 |日 |(周一) |(周二) | (周三) | (周四) | (周五) | (周六) \"\"\", re.VERBOSE) #Drop Date,\n",
    "    decimal_regex = re.compile(r\"[^a-zA-Z]\\d+\") #Number,\n",
    "    space_regex = re.compile(r\"\\s+\")     # Space,\n",
    "    regEx = \"[\\n”“|,，；;''/?! 。的了是]\"  # Dropb: \\n、中文符號、|，要刪什麼就寫什麼\n",
    "    line = username_regex.sub(r\"\", line)\n",
    "    line = url_regex.sub(r\"\", line)\n",
    "    line = data_regex.sub(r\"\", line)\n",
    "    line = decimal_regex.sub(r\"\", line)\n",
    "    line = space_regex.sub(r\"\", line)\n",
    "    return line\n",
    "\n",
    "data['mess2'] = data['mess1'].apply(drop_numbers)\n",
    "data['mess2'] = data['mess2'].apply(drop_url)\n",
    "data['mess2'] = data['mess2'].apply(drop_en)\n",
    "#data['mess2'] = data['mess2'].apply(translate)  #run long time\n",
    "#data['mess2'] = data['mess2'].apply(regex_change)  #reference 自定義\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stopwords & Jieba 解構 方式刪除\n",
    "from collections import Counter\n",
    "import jieba\n",
    "import json\n",
    "\n",
    "stopword = \"input/NLP-stopwords.dat\" \n",
    "outputword_file = \"input/NLP-CN_words.json\"\n",
    "\n",
    "def read_file(file_name):\n",
    "    fp = open(file_name, \"r\", encoding=\"utf-8\")\n",
    "    content_lines = fp.readlines()\n",
    "    fp.close()\n",
    "    for i in range(len(content_lines)):\n",
    "        content_lines[i] = content_lines[i].rstrip(\"\\n\")\n",
    "    return content_lines\n",
    "\n",
    "def save_file(file_name, content):\n",
    "    fp = open(file_name, \"w\", encoding=\"utf-8\")\n",
    "    fp.write(content)\n",
    "    fp.close()\n",
    "    \n",
    "def delete_stopwords(lines):\n",
    "    stopwords = read_file(stopword)  #Filter Stopword\n",
    "    all_words = []\n",
    "    jieba.load_userdict('input/NLP-userdictCN.txt')   #Filter userdict\n",
    "    \n",
    "    # jieba.cut(line, cut_all=True)  全模式 \n",
    "    # jieba.cut(line, cut_all=False) 精確模式(預設) \n",
    "    for line in lines:\n",
    "        all_words += [word for word in jieba.cut(line) if word not in stopwords]\n",
    "        #for word in jieba.cut(line):\n",
    "        #    if word not in stopwords:\n",
    "        #        print(word)\n",
    "        \n",
    "    dict_words = dict(Counter(all_words))\n",
    "    return dict_words,all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/9q/486czkcn7lv5v0hwbt71twdc0000gn/T/jieba.cache\n",
      "Loading model cost 0.517 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>台灣</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>死亡</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>重症</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>台湾</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>你們</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Word  Counts\n",
       "0   台灣     206\n",
       "1   死亡     193\n",
       "2   重症     149\n",
       "3   台湾     124\n",
       "4   你們      73"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the NLP　Action (clean word)\n",
    "title = []\n",
    "counts = []\n",
    "filter_word=[]\n",
    "sorted_list =[]\n",
    "\n",
    "#run fun(delete_stopwords) jieba and sorted\n",
    "jieba_word,dd = delete_stopwords(data['mess2'].tolist()) \n",
    "sorted_set = sorted(jieba_word.items(), key=lambda d:d[1], reverse=True)  \n",
    "#print(sorted_set)   \n",
    "\n",
    "\n",
    "# convert (Set) to (list) and Sort\n",
    "for words in sorted_set[:]:  \n",
    "    wordList = list(words)\n",
    "    sorted_list += [wordList]\n",
    "#print(sorted_list)\n",
    "\n",
    "\n",
    "# Convert (List) To (Table)\n",
    "for j in range(len(sorted_list)): \n",
    "    a = len(sorted_list[j][0])\n",
    "    if a> 1: \n",
    "        title.append(sorted_list[j][0])\n",
    "        counts.append(sorted_list[j][1])\n",
    "\n",
    "dicts = {\"a\":title,\"b\":counts}\n",
    "count_data = pd.DataFrame((zip(title,counts)),columns = ['Word','Counts'])\n",
    "count_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7fc753320f10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(font_path=\"/System/Library/fonts/PingFang.ttc\",width=2000, height=1200)\n",
    "wc.generate(str(dd))\n",
    "wc.to_file(\"input/NLP-wordcloud.jpg\")\n",
    "\n",
    "# https://iter01.com/569982.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Titles', 'TopWord']\n"
     ]
    }
   ],
   "source": [
    "wb.create_sheet(\"TopWord\",1) \n",
    "wsM = wb[\"TopWord\"] \n",
    "\n",
    "for r in dataframe_to_rows(count_data, index=True, header=True):\n",
    "    wsM.append(r)\n",
    "\n",
    "wsM[\"K1\"] = \"The Top Word output\"\n",
    "wsM[\"K2\"] = str(datetime.date.today())\n",
    "\n",
    "wb.save(\"input/NLP-save1.xlsx\")\n",
    "print(wb.sheetnames)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Text Polarity (情感分析)\n",
    "\n",
    ">It is the expression that determines the sentimental aspect of an opinion. In textual data, the result of sentiment analysis can be determined for each entity in the sentence, document or sentence. The sentiment polarity can be determined as positive, negative and neutral.\n",
    "\n",
    "它是決定意見的情感方面的表達方式。 在文本數據中，可以針對句子、文檔或句子中的每個實體確定情感分析的結果。 情緒極性可以確定為正面、負面和中性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob import TextBlob\n",
    "\n",
    "# def get_polarity(text):\n",
    "#     textblob = TextBlob(str(text.encode('utf-8')))\n",
    "#     pol = textblob.sentiment.polarity\n",
    "#     return pol\n",
    "\n",
    "# data['polarity'] = data['mess1'].apply(get_polarity)\n",
    "# data['polarity'].head()\n",
    "# #情感分析不適用於中文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Subjectivity 文本主觀性\n",
    "\n",
    ">In natural language, subjectivity refers to expression of opinions, evaluations, feelings, and speculations and thus incorporates sentiment. Subjective text is further classified with sentiment or polarity.\n",
    "\n",
    "在自然語言中，主觀性是指意見、評價、感受和推測的表達，因此包含了情感。 主觀文本進一步分類為情感或極性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets calculate the Subjectvity of the Reviews\n",
    "# def get_subjectivity(text):\n",
    "#     textblob = TextBlob(str(text.encode('utf-8')))\n",
    "#     subj = textblob.sentiment.subjectivity\n",
    "#     return subj\n",
    "\n",
    "# data['subjectivity'] = data['mess1'].apply(get_subjectivity)\n",
    "# data['subjectivity'].head()\n",
    "# #情感分析不適用於中文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob import TextBlob\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[['polarity','subjectivity']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Visualizing Polarity and Subjectivity\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# sns.distplot(data['polarity'])\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# sns.distplot(data['subjectivity'])\n",
    "\n",
    "# plt.suptitle('Distribution of Polarity and Subjectivity')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 留言次數計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nam</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>网上冲浪李老八</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>許立良</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>許遵榮</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>朱寶寶</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>劉沛</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nam    0\n",
       "0  网上冲浪李老八  108\n",
       "1      許立良   98\n",
       "2      許遵榮   72\n",
       "3      朱寶寶   45\n",
       "4       劉沛   45"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = data.groupby(\"nam\").size().sort_values(ascending=False).reset_index()\n",
    "dtc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Titles', 'TopWord', 'MostComment']\n"
     ]
    }
   ],
   "source": [
    "wb.create_sheet(\"MostComment\",2) \n",
    "wsC = wb[\"MostComment\"] \n",
    "\n",
    "for r in dataframe_to_rows(dtc, index=True, header=True):\n",
    "    wsC.append(r)\n",
    "    \n",
    "wsC[\"K1\"] = \"The Most Comment output\"\n",
    "wsC[\"K2\"] = str(datetime.date.today())\n",
    "\n",
    "wb.save(\"input/NLP-save1.xlsx\")\n",
    "print(wb.sheetnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
