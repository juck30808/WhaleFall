{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [8, 4.5]\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn \n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning Models in Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating advanced classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chapter_9_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-825ac8429b40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mchapter_9_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mperformance_evaluation_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexport_graphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chapter_9_utils'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from chapter_9_utils import performance_evaluation_report\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('credit_card_default.csv', index_col=0, na_values='')\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop('default_payment_next_month')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   test_size=0.2, \n",
    "                                                   stratify=y, \n",
    "                                                   random_state=42)\n",
    "\n",
    "num_features = X_train.select_dtypes(include='number').columns.to_list()\n",
    "cat_features = X_train.select_dtypes(include='object').columns.to_list()\n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "cat_list = [list(X_train[column].dropna().unique()) for column in cat_features]\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(categories=cat_list, sparse=False, \n",
    "                            handle_unknown='error', drop='first'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('numerical', num_pipeline, num_features),\n",
    "    ('categorical', cat_pipeline, cat_features)],\n",
    "                                 remainder='drop')\n",
    "\n",
    "tree_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', DecisionTreeClassifier(random_state=42))])\n",
    "\n",
    "tree_pipeline.fit(X_train, y_train)\n",
    "\n",
    "LABELS = ['No Default', 'Default']\n",
    "tree_perf = performance_evaluation_report(tree_pipeline, X_test, \n",
    "                                         y_test, labels=LABELS, \n",
    "                                         show_plot=True, \n",
    "                                         show_pr_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the depth of the tree\n",
    "tree_classifier = tree_pipeline.named_steps['classifier']\n",
    "tree_classifier.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomForestClassifier, \n",
    "                             GradientBoostingClassifier)\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a Random Forest Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6df2c4b637c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n\u001b[0;32m      3\u001b[0m                              \u001b[1;33m(\u001b[0m\u001b[1;34m'classifier'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m ])\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                             ('classifier', rf)\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "rf_perf = performance_evaluation_report(rf_pipeline, X_test, \n",
    "                                       y_test, labels=LABELS, \n",
    "                                       show_plot=True,\n",
    "                                       show_pr_curve=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a Gradient Boosting Trees Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt =  GradientBoostingClassifier(random_state=42)\n",
    "gbt_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', gbt)\n",
    "])\n",
    "\n",
    "gbt_pipeline.fit(X_train, y_train)\n",
    "gbt_perf = performance_evaluation_report(gbt_pipeline, X_test, \n",
    "                                        y_test, labels=LABELS, \n",
    "                                        show_plot=True,\n",
    "                                        show_pr_curve=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create a xgBoost Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state=42)\n",
    "xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', xgb)\n",
    "])\n",
    "\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "xgb_perf = performance_evaluation_report(xgb_pipeline, X_test, \n",
    "                                        y_test, labels=LABELS, \n",
    "                                        show_plot=True,\n",
    "                                        show_pr_curve=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create a LightGBM classifier Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(random_state=42)\n",
    "lgbm_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', lgbm)\n",
    "])\n",
    "\n",
    "lgbm_pipeline.fit(X_train, y_train)\n",
    "lgbm_perf = performance_evaluation_report(lgbm_pipeline, X_test, \n",
    "                                         y_test, labels=LABELS, \n",
    "                                         show_plot=True,\n",
    "                                         show_pr_curve=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's more about use advanced classifiers to achieve better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we go over the most important hyperparameters of the considered models and show a possible way of tuning them using Randomized Search. With more complex models, the training time is significantly longer than with the basic Decision Tree, so we need to find a balance between the time we want to spend on tuning the hyperparameters and the expected results. Also, bear in mind that changing the values of some parameters (such as learning rate or the number of estimators) can itself influence the training time of the models.\n",
    "\n",
    "To have the results in a reasonable amount of time, we used the Randomized Search with 100 different sets of hyperparameters for each model (the number of actually fitted models is higher due to cross-validation). Just as in the recipe *Grid Search and Cross-Validation*, we used recall as the criterion for selecting the best model. Additionally, we used the scikit-learn compatible APIs of XGBoost and LightGBM to make the process as easy to follow as possible. For a complete list of hyperparameters and their meaning, please refer to corresponding documentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "N_SEARCHES = 100\n",
    "k_fold = StratifiedKFold(5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When tuning the Random Forest classifier, we look at the following hyperparameters (there are more available for tuning):\n",
    "* `n_estimators` - the number of decision trees in a forest. The goal is to find a balance between improved accuracy and computational cost.\n",
    "* `max_features` - the maximum number of features considered for splitting a node. The default is the square root of the number of features. When None, all features are considered.\n",
    "* `max_depth` - the maximum number of levels in each decision tree\n",
    "* `min_samples_split` - the minimum number of observations required to split each node. When set to high it may cause underfitting, as the trees will not split enough times.\n",
    "* `min_samples_leaf` - the minimum number of data points allowed in a leaf. Too small a value might cause overfitting, while large values might prevent the tree from growing and cause underfitting.\n",
    "* `bootstrap` - whether to use bootstrapping for each tree in the forest\n",
    "    \n",
    "    We define the grid below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {'classifier__n_estimators': np.linspace(100, 1000, 10, dtype=int),\n",
    "                'classifier__max_features': ['log2', 'sqrt', None],\n",
    "                'classifier__max_depth': np.arange(3, 11, 1, dtype=int),\n",
    "                'classifier__min_samples_split': [2, 5, 10],\n",
    "                'classifier__min_samples_leaf': np.arange(1, 51, 2, dtype=int),\n",
    "                'classifier__bootstrap': [True, False]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use the randomized search to tune the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a4daf427005c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m rf_rs =  RandomizedSearchCV(rf_pipeline, rf_param_grid, scoring='recall', \n\u001b[0m\u001b[0;32m      2\u001b[0m                            \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                             n_iter=N_SEARCHES, random_state=42)\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrf_rs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "rf_rs =  RandomizedSearchCV(rf_pipeline, rf_param_grid, scoring='recall', \n",
    "                           cv=k_fold, n_jobs=-1, verbose=1, \n",
    "                            n_iter=N_SEARCHES, random_state=42)\n",
    "\n",
    "rf_rs.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {rf_rs.best_params_}') \n",
    "print(f'Recall (Training set): {rf_rs.best_score_:.4f}') \n",
    "print(f'Recall (Test set): {metrics.recall_score(y_test, rf_rs.predict(X_test)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_rs_perf = performance_evaluation_report(rf_rs, X_test, \n",
    "                                          y_test, labels=LABELS, \n",
    "                                          show_plot=True,\n",
    "                                          show_pr_curve=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosted Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Gradient Boosted Trees are also an ensemble method built on top of decision trees, a lot of the parameters are the same as in the case of the Random Forest. The new one is the learning rate, which is used in the gradient descent algorithm to control the rate of descent towards the minimum of the loss function. When tuning the tree manually, we should consider this hyperparameter together with the number of estimators, as reducing the learning rate (the learning is slower), while increasing the number of estimators can increase the computation time significantly.\n",
    "\n",
    "We define the grid as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_param_grid = {'classifier__n_estimators': np.linspace(100, 1000, 10, dtype=int),\n",
    "                 'classifier__learning_rate': np.arange(0.05, 0.31, 0.05),\n",
    "                 'classifier__max_depth': np.arange(3, 11, 1, dtype=int),\n",
    "                 'classifier__min_samples_split': np.linspace(0.1, 0.5, 12),\n",
    "                 'classifier__min_samples_leaf': np.arange(1, 51, 2, dtype=int),\n",
    "                 'classifier__max_features':['log2', 'sqrt', None]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the randomized search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gbt_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e4d79ba64472>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m gbt_rs =  RandomizedSearchCV(gbt_pipeline, gbt_param_grid, scoring='recall', \n\u001b[0m\u001b[0;32m      2\u001b[0m                             \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                             n_iter=N_SEARCHES, random_state=42)\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgbt_rs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gbt_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "gbt_rs =  RandomizedSearchCV(gbt_pipeline, gbt_param_grid, scoring='recall', \n",
    "                            cv=k_fold, n_jobs=-1, verbose=1, \n",
    "                            n_iter=N_SEARCHES, random_state=42)\n",
    "\n",
    "gbt_rs.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {gbt_rs.best_params_}') \n",
    "print(f'Recall (Training set): {gbt_rs.best_score_:.4f}') \n",
    "print(f'Recall (Test set): {metrics.recall_score(y_test, gbt_rs.predict(X_test)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_rs_perf = performance_evaluation_report(gbt_rs, X_test, \n",
    "                                           y_test, labels=LABELS, \n",
    "                                           show_plot=True,\n",
    "                                           show_pr_curve=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn API of XGBoost makes sure that the hyperparameters are named similarly to their equivalents other scikit-learn's classifiers. So the XGBoost native eta hyperparameter is called learning_rate in scikit-learn's API. \n",
    "\n",
    "The new hyperparameters we consider for this example are:\n",
    "* `min_child_weight` - indicates the minimum sum of weights of all observations required in a child. This hyperparameter is used for controlling overfitting. Cross-validation should be used for tuning.\n",
    "* `colsample_bytree` - indicates the fraction of columns to be randomly sampled for each tree.\n",
    "    \n",
    "    We define the grid as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_grid = {'classifier__n_estimators': np.linspace(100, 1000, 10, dtype=int),\n",
    "                 'classifier__learning_rate': np.arange(0.05, 0.31, 0.05),\n",
    "                 'classifier__max_depth': np.arange(3, 11, 1, dtype=int),\n",
    "                 'classifier__min_child_weight': np.arange(1, 8, 1, dtype=int),\n",
    "                 'classifier__colsample_bytree': np.linspace(0.3, 1, 7)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For defining ranges of parameters that are restricted (such as colsample_bytree which cannot be higher than 1.0) it is better to use `np.linspace` rather than `np.arange`, because the latter allows for some inconsistencies when the step is defined as floating-point. For example, the last value might be 1.0000000002, which then causes an error while training the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_rs =  RandomizedSearchCV(xgb_pipeline, xgb_param_grid, scoring='recall', \n",
    "                            cv=k_fold, n_jobs=-1, verbose=1, \n",
    "                            n_iter=N_SEARCHES, random_state=42)\n",
    "\n",
    "xgb_rs.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {xgb_rs.best_params_}') \n",
    "print(f'Recall (Training set): {xgb_rs.best_score_:.4f}') \n",
    "print(f'Recall (Test set): {metrics.recall_score(y_test, xgb_rs.predict(X_test)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_rs_perf = performance_evaluation_report(xgb_rs, X_test, \n",
    "                                           y_test, labels=LABELS, \n",
    "                                           show_plot=True,\n",
    "                                           show_pr_curve=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LightGBM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tune the same parameters as in XGBoost, though more is definitely possible and encouraged. The grid is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_param_grid = {'classifier__n_estimators': np.linspace(100, 1000, 10, dtype=int),\n",
    "                  'classifier__learning_rate': np.arange(0.05, 0.31, 0.05),\n",
    "                  'classifier__max_depth': np.arange(3, 11, 1, dtype=int),\n",
    "                  'classifier__colsample_bytree': np.linspace(0.3, 1, 7)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_rs =  RandomizedSearchCV(lgbm_pipeline, lgbm_param_grid, scoring='recall', \n",
    "                             cv=k_fold, n_jobs=-1, verbose=1, \n",
    "                             n_iter=N_SEARCHES, random_state=42)\n",
    "\n",
    "lgbm_rs.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {lgbm_rs.best_params_}') \n",
    "print(f'Recall (Training set): {lgbm_rs.best_score_:.4f}') \n",
    "print(f'Recall (Test set): {metrics.recall_score(y_test, lgbm_rs.predict(X_test)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_rs_perf = performance_evaluation_report(lgbm_rs, X_test, \n",
    "                                            y_test, labels=LABELS, \n",
    "                                            show_plot=True,\n",
    "                                            show_pr_curve=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we present a summary of all the classifiers we have considered in the last 3 recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree_perf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-212b9448f777>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m results_dict = {'decision_tree_baseline': tree_perf,\n\u001b[0m\u001b[0;32m      2\u001b[0m                \u001b[1;34m'random_forest'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrf_perf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                \u001b[1;34m'random_forest_rs'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrf_rs_perf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                \u001b[1;34m'gradient_boosted_trees'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgbt_perf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                \u001b[1;34m'gradient_boosted_trees_rs'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgbt_rs_perf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tree_perf' is not defined"
     ]
    }
   ],
   "source": [
    "results_dict = {'decision_tree_baseline': tree_perf,\n",
    "               'random_forest': rf_perf,\n",
    "               'random_forest_rs': rf_rs_perf,\n",
    "               'gradient_boosted_trees': gbt_perf,\n",
    "               'gradient_boosted_trees_rs': gbt_rs_perf,\n",
    "               'xgboost': xgb_perf,\n",
    "               'xgboost_rs': xgb_rs_perf,\n",
    "               'light_gbm': lgbm_perf,\n",
    "               'light_gbm_rs': lgbm_rs_perf}\n",
    "\n",
    "results_comparison = pd.DataFrame(results_dict).T\n",
    "results_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results_comparison.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e2c69ea5e41e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults_comparison\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'results_comparison.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mresults_comparison\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'model'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mresults_comparison\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results_comparison.csv'"
     ]
    }
   ],
   "source": [
    "results_comparison = pd.read_csv('results_comparison.csv')\n",
    "results_comparison.rename(columns={'Unnamed: 0': 'model'}, inplace=True)\n",
    "results_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using stacking for improved performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import (train_test_split,\n",
    "                                    StratifiedKFold)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load and preprocess data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "k_fold = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "df = pd.read_csv('credit_card_fraud.csv')\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop('Class')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   test_size=0.2, \n",
    "                                                   stratify=y, \n",
    "                                                   random_state=RANDOM_STATE)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define a list of classifiers to consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_list = [('dec_tree', DecisionTreeClassifier(random_state=RANDOM_STATE)),\n",
    "           ('log_reg', LogisticRegression()),\n",
    "           ('knn', KNeighborsClassifier()),\n",
    "           ('naive_bayes', GaussianNB())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Iterate over the selected models, fit them to the data and calculate recall using the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_tuple in clf_list:\n",
    "    model = model_tuple[1]\n",
    "    if 'random_state' in model.get_params().keys():\n",
    "        model.set_params(random_state=RANDOM_STATE)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    recall = metrics.recall_score(y_pred, y_test)\n",
    "    print(f\"{model_tuple[0]}'s recall score: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Define and fit the stacking classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "stack_clf = StackingClassifier(clf_list, \n",
    "                              final_estimator=lr,\n",
    "                              cv=k_fold,\n",
    "                              n_jobs=-1)\n",
    "stack_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create predictions and evaluate the stacked ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = stack_clf.predict(X_test)\n",
    "recall = metrics.recall_score(y_pred, y_test)\n",
    "print(f\"The stacked ensemble's recall score: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the code for the *Investigating advanced classifiers* recipe before this one (you do not need to run the *There's more* section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting eli5\n",
      "  Downloading eli5-0.11.0-py2.py3-none-any.whl (106 kB)\n",
      "Requirement already satisfied: six in c:\\users\\cti110016\\anaconda3\\lib\\site-packages (from eli5) (1.15.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cti110016\\anaconda3\\lib\\site-packages (from eli5) (2.11.2)\n",
      "Requirement already satisfied: graphviz in c:\\users\\cti110016\\anaconda3\\lib\\site-packages (from eli5) (0.19.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\cti110016\\anaconda3\\lib\\site-packages (from eli5) (1.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\cti110016\\anaconda3\\lib\\site-packages (from eli5) (1.5.2)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\cti110016\\anaconda3\\lib\\site-packages (from eli5) (1.19.2)\n",
      "Requirement already satisfied: attrs>16.0.0 in c:\\users\\cti110016\\anaconda3\\lib\\site-packages (from eli5) (20.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\cti110016\\anaconda3\\lib\\site-packages (from jinja2->eli5) (2.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\cti110016\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20->eli5) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\cti110016\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20->eli5) (1.0.1)\n",
      "Installing collected packages: tabulate, eli5\n",
      "Successfully installed eli5-0.11.0 tabulate-0.8.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.inspection import permutation_importance\n",
    "from sklearn.base import clone \n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Extract the classifier and preprocessor from the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-feb22c4d53f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# rf_pipeline = rf_rs.best_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrf_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'classifier'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'preprocessor'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# in case we have the fitted grid search object `rf_rs`, we extract the best pipeline\n",
    "# rf_pipeline = rf_rs.best_estimator_\n",
    "\n",
    "rf_classifier = rf_pipeline.named_steps['classifier']\n",
    "preprocessor = rf_pipeline.named_steps['preprocessor']\n",
    "\n",
    "# in case we want to manually assign hyperparameters based on previous grid search\n",
    "# best_parameters =  {'n_estimators': 400, 'min_samples_split': 2, \n",
    "#                     'min_samples_leaf': 49, 'max_features': None, \n",
    "#                     'max_depth': 20, 'bootstrap': True, 'random_state': 42}\n",
    "# rf_classifier = rf_classifier.set_params(**best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Recover feature names from the preprocessing transformer and transform the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = preprocessor.named_transformers_['categorical'] \\\n",
    "                         .named_steps['onehot'] \\\n",
    "                         .get_feature_names(\n",
    "    input_features=cat_features\n",
    ")\n",
    "feat_names = np.r_[num_features, feat_names]\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(\n",
    "    preprocessor.transform(X_train), \n",
    "    columns=feat_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Extract the default feature importance and calculate the cumulative importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_feat_imp = pd.DataFrame(rf_classifier.feature_importances_,\n",
    "                          index=feat_names,\n",
    "                          columns=['mdi'])\n",
    "rf_feat_imp = rf_feat_imp.sort_values('mdi', ascending=False)\n",
    "rf_feat_imp['cumul_importance_mdi'] = np.cumsum(rf_feat_imp.mdi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Define a function for plotting top X features in terms of their importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_important_features(feat_imp, method='MDI', \n",
    "                                n_features=10, bottom=False):\n",
    "    '''\n",
    "    Function for plotting the top/bottom x features in terms of their importance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feat_imp : pd.Series\n",
    "    A pd.Series with calculated feature importances\n",
    "    method : str\n",
    "    A string representing the method of calculating the importances.\n",
    "    Used for the title of the plot.\n",
    "    n_features : int\n",
    "    Number of top/bottom features to plot\n",
    "    bottom : boolean\n",
    "    Indicates if the plot should contain the bottom feature importances.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ax : matplotlib.axes._subplots.AxesSubplot\n",
    "    Ax cointaining the plot\n",
    "    '''''\n",
    "    \n",
    "    if bottom:\n",
    "        indicator = 'Bottom'\n",
    "        feat_imp = feat_imp.sort_values(ascending=True)\n",
    "    else:\n",
    "        indicator = 'Top'\n",
    "        feat_imp = feat_imp.sort_values(ascending=False)\n",
    "    ax = feat_imp.head(n_features).plot.barh()\n",
    "    ax.invert_yaxis()\n",
    "    ax.set(title=('Feature importance - '\n",
    "                  f'{method} ({indicator} {n_features})'), \n",
    "           xlabel='Importance', \n",
    "           ylabel='Feature')\n",
    "            \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_important_features(rf_feat_imp.mdi, \n",
    "                            method='MDI')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Plot the cumulative importance of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = range(len(feat_names))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x_values, rf_feat_imp.cumul_importance_mdi, 'b-')\n",
    "ax.hlines(y = 0.95, xmin=0, xmax=len(x_values), \n",
    "         color = 'g', linestyles = 'dashed')\n",
    "ax.set(title='Cumulative Importances', \n",
    "      xlabel='Variable', \n",
    "      ylabel='Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Top 10 features account for {100 * rf_feat_imp.head(10).mdi.sum():.2f}% of the total importance.')\n",
    "print(f'Top {rf_feat_imp[rf_feat_imp.cumul_importance_mdi <= 0.95].shape[0]} features account for 95% of importance.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Calculate and plot permutation importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(rf_classifier, n_iter = 25, \n",
    "                            random_state=42)\n",
    "perm.fit(X_train_preprocessed, y_train)\n",
    "rf_feat_imp['permutation'] = perm.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_important_features(rf_feat_imp.permutation, \n",
    "                            method='Permutation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Define a function for calculating the drop-column feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_col_feat_imp(model, X, y, random_state = 42):\n",
    "    '''\n",
    "    Function for calculating the drop column feature importance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : scikit-learns model\n",
    "    Object representing the estimator with selected hyperparameters.\n",
    "    X : pd.DataFrame\n",
    "    Features for training the model\n",
    "    y : pd.Series\n",
    "    The target\n",
    "    random_state : int\n",
    "    Random state for reproducibility\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    importances : list\n",
    "    List containing the calculated feature importances in the order of appearing in X\n",
    "    \n",
    "    '''''\n",
    "    \n",
    "    model_clone = clone(model)\n",
    "    model_clone.random_state = random_state\n",
    "    model_clone.fit(X, y)\n",
    "    benchmark_score = model_clone.score(X, y)\n",
    "    \n",
    "    importances = []\n",
    "    \n",
    "    for col in X.columns:\n",
    "        model_clone = clone(model)\n",
    "        model_clone.random_state = random_state\n",
    "        model_clone.fit(X.drop(col, axis = 1), y)\n",
    "        drop_col_score = model_clone.score(X.drop(col, axis = 1), \n",
    "                                          y)\n",
    "        importances.append(benchmark_score - drop_col_score)\n",
    "        \n",
    "    return importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Calculate and plot the drop-column feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_feat_imp['drop_column'] = drop_col_feat_imp(\n",
    "    rf_classifier, \n",
    "    X_train_preprocessed, \n",
    "    y_train, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_important_features(rf_feat_imp.drop_column, \n",
    "                            method='Drop column')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_important_features(rf_feat_imp.drop_column, \n",
    "                            method='Drop column', \n",
    "                            bottom=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating different approaches to handling imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "from chapter_9_utils import performance_evaluation_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load and prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('credit_card_fraud.csv')\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop('Class')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   test_size=0.2, \n",
    "                                                   stratify=y, \n",
    "                                                   random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_perf = performance_evaluation_report(rf, X_test, y_test, \n",
    "                                       show_plot=True, \n",
    "                                       show_pr_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Undersample the data and train a Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "X_rus, y_rus = rus.fit_resample(X_train, y_train)\n",
    "print(f'The new class proportions are: {dict(Counter(y_rus))}')\n",
    "\n",
    "rf.fit(X_rus, y_rus)\n",
    "rf_rus_perf = performance_evaluation_report(rf, X_test, y_test, \n",
    "                                           show_plot=True, \n",
    "                                           show_pr_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_rus_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Oversample the data and train a Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "X_ros, y_ros = ros.fit_resample(X_train, y_train)\n",
    "print(f'The new class proportions are: {dict(Counter(y_ros))}')\n",
    "\n",
    "rf.fit(X_ros, y_ros)\n",
    "rf_ros_perf = performance_evaluation_report(rf, X_test, y_test, \n",
    "                                           show_plot=True, \n",
    "                                           show_pr_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_ros_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Oversample using SMOTE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smote, y_smote = SMOTE(random_state=RANDOM_STATE).fit_resample(X_train, y_train)\n",
    "print(f'The new class proportions are: {dict(Counter(y_smote))}')\n",
    "rf.fit(X_smote, y_smote)\n",
    "rf_smote_perf = performance_evaluation_report(rf, X_test, y_test, \n",
    "                                             show_plot=True, \n",
    "                                             show_pr_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_smote_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Oversample using ADASYN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adasyn, y_adasyn = ADASYN(random_state=RANDOM_STATE).fit_resample(X_train, y_train)\n",
    "print(f'The new class proportions are: {dict(Counter(y_adasyn))}')\n",
    "rf.fit(X_adasyn, y_adasyn)\n",
    "rf_adasyn_perf = performance_evaluation_report(rf, X_test, y_test, \n",
    "                                              show_plot=True, \n",
    "                                              show_pr_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_adasyn_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Use sample weights in the Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cw = RandomForestClassifier(random_state=RANDOM_STATE, \n",
    "                              class_weight='balanced',\n",
    "                              n_jobs=-1)\n",
    "rf_cw.fit(X_train, y_train)\n",
    "rf_cw_perf = performance_evaluation_report(rf_cw, X_test, y_test, \n",
    "                                          show_plot=True, \n",
    "                                          show_pr_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cw_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import (cross_val_score, \n",
    "                                    StratifiedKFold)\n",
    "from lightgbm import LGBMClassifier\n",
    "from chapter_9_utils import performance_evaluation_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define parameters for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 5\n",
    "MAX_EVALS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load and prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('credit_card_fraud.csv')\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop('Class')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Define the objective function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params, n_folds = N_FOLDS, random_state=42):\n",
    "    \n",
    "    model = LGBMClassifier(**params)\n",
    "    model.set_params(random_state=random_state)\n",
    "    \n",
    "    k_fold = StratifiedKFold(n_folds, shuffle=True, \n",
    "                             random_state=random_state)\n",
    "    \n",
    "    metrics = cross_val_score(model, X_train, y_train, \n",
    "                              cv=k_fold, scoring='recall')\n",
    "    loss = -1 * metrics.mean()\n",
    "    \n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Define the search space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_param_grid = {\n",
    "    'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "    'max_depth': hp.choice('max_depth', [-1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),\n",
    "    'n_estimators': hp.choice('n_estimators', [10, 50, 100, \n",
    "                                               300, 750, 1000]),\n",
    "    'is_unbalance': hp.choice('is_unbalance', [True, False]),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1),\n",
    "    'learning_rate': hp.uniform ('learning_rate', 0.05, 0.3),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Run the Bayesian optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best_set = fmin(fn= objective,\n",
    "                space= lgbm_param_grid,\n",
    "                algo= tpe.suggest,\n",
    "                max_evals = MAX_EVALS,\n",
    "                trials= trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load if already finished the search\n",
    "#best_set = pickle.load(open('best_set.p', 'rb'))\n",
    "best_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Define the dictionaries for mapping the results to hyperparameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosting_type = {0: 'gbdt', 1: 'dart', 2: 'goss'}\n",
    "max_depth = {0: -1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, \n",
    "            6: 7, 7: 8, 8: 9, 9: 10}\n",
    "n_estimators = {0: 10, 1: 50, 2: 100, 3: 300, 4: 750, 5: 1000}\n",
    "is_unbalance = {0: True, 1: False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Fit a model using the best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_param_grid = {'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "                  'max_depth': hp.choice('max_depth', [-1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),\n",
    "                  'n_estimators': hp.choice('n_estimators', [10, 50, 100, 300, 750, 1000]),\n",
    "                  'is_unbalance': hp.choice('is_unbalance', [True, False]),\n",
    "                  'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1),\n",
    "                  'learning_rate': hp.uniform ('learning_rate', 0.05, 0.3),\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lgbm = LGBMClassifier(\n",
    "boosting_type = boosting_type[best_set['boosting_type']], \n",
    "max_depth = max_depth[best_set['max_depth']], \n",
    "n_estimators = n_estimators[best_set['n_estimators']], \n",
    "is_unbalance = is_unbalance[best_set['is_unbalance']],\n",
    "colsample_bytree = best_set['colsample_bytree'], \n",
    "learning_rate = best_set['learning_rate']\n",
    ")\n",
    "best_lgbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = performance_evaluation_report(best_lgbm, X_test, y_test, \n",
    "                                 show_plot=True, \n",
    "                                 show_pr_curve=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "trials = pickle.load(open(\"trials_final.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Parse all the information from `trials.results` into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(trials.results)\n",
    "params_df = json_normalize(results_df['params'])\n",
    "\n",
    "results_df = pd.concat([results_df.drop('params', axis=1), params_df], \n",
    "                      axis=1)\n",
    "results_df['iteration'] = np.arange(len(results_df)) + 1\n",
    "results_df.sort_values('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Draw sample from the selected distribution of `colsample_bytree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsample_bytree_dist = []\n",
    "\n",
    "for _ in range(10000):\n",
    "    x = sample(lgbm_param_grid['colsample_bytree'])\n",
    "    colsample_bytree_dist.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (16, 8))\n",
    "\n",
    "sns.kdeplot(colsample_bytree_dist, \n",
    "           label='Sampling Distribution', \n",
    "           ax=ax[0])\n",
    "sns.kdeplot(results_df['colsample_bytree'], \n",
    "           label='Bayesian Optimization', \n",
    "           ax=ax[0])\n",
    "ax[0].set(title='Distribution of colsample_bytree', \n",
    "         xlabel='Value',\n",
    "         ylabel='Density')\n",
    "ax[0].legend()\n",
    "\n",
    "sns.regplot('iteration', 'colsample_bytree', \n",
    "           data=results_df, ax=ax[1])\n",
    "ax[1].set(title='colsample_bytree over Iterations', \n",
    "         xlabel='Iteration', \n",
    "         ylabel='Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Plot the distribution of `n_estimators`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['n_estimators'].value_counts() \\\n",
    "                          .plot \\\n",
    "                          .bar(title=('# of Estimators' \n",
    "                                       ' Distribution'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Plot the evolution of the observed losses over iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(results_df.iteration, results_df.loss, 'o')\n",
    "ax.set(title='TPE Sequence of Losses', \n",
    "      xlabel='Iteration',\n",
    "      ylabel='Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
