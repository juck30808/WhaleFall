{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [机器学习](05.00-Machine-Learning.ipynb) | [目录](Index.ipynb) | [Scikit-Learn简介](05.02-Introducing-Scikit-Learn.ipynb) >\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/wangyingsm/Python-Data-Science-Handbook/blob/master/notebooks/05.01-What-Is-Machine-Learning.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "> In many ways, machine learning is the primary means by which data science manifests itself to the broader world.\n",
    "Machine learning is where these computational and algorithmic skills of data science meet the statistical thinking of data science, and the result is a collection of approaches to inference and data exploration that are not about effective theory so much as effective computation.\n",
    "\n",
    "在很多情況下，機器學習是數據科學本身以及更廣泛領域中的主要方法。機器學習是數據科學中計算及算法和統計思維相結合的產物，結果得到的是一整套的推理方法和數據分析工具。\n",
    "\n",
    "> The term \"machine learning\" is sometimes thrown around as if it is some kind of magic pill: *apply machine learning to your data, and all your problems will be solved!* As you might expect, the reality is rarely this simple. While these methods can be incredibly powerful, to be effective they must be approached with a firm grasp of the strengths and weaknesses of each method, as well as a grasp of general concepts such as bias and variance, overfitting and underfitting, and more.Fundamentally, machine learning involves building mathematical models to help understand data.\n",
    "\"Learning\" enters the fray when we give these models *tunable parameters* that can be adapted to observed data; in this way the program can be considered to be \"learning\" from the data.\n",
    "Once these models have been fit to previously seen data, they can be used to predict and understand aspects of newly observed data.\n",
    "\n",
    "“機器學習”有時候會被濫用就好像萬靈丹：你必須掌握每種方法的優缺點才能令它們更有效，你需要掌握偏差和方差的基本概念，以及過擬合和欠擬合等等。機器學習基本上就是關於構建數學模型來幫助我們理解數據。當我們為這些模型提供了*可調整的參數*時，“學習”能讓我們從觀察到的數據中調整這些參數。也就是說，這個過程可以被認為我們從數據中“學習”。一旦這些模型已經適應（擬合）了觀察到的數據之後，它們就可以用來預測和理解新的數據。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories of Machine Learning (Supervised & Unsupervised)\n",
    "\n",
    "> *Supervised learning* involves somehow modeling the relationship between measured features of data and some label associated with the data; once this model is determined, it can be used to apply labels to new, unknown data.\n",
    "This is further subdivided into *classification* tasks and *regression* tasks: in classification, the labels are discrete categories, while in regression, the labels are continuous quantities.\n",
    "We will see examples of both types of supervised learning in the following section.\n",
    "\n",
    "*有監督學習*指的是在除了數據本身外，我們還擁有對數據進行的標記，有監督學習就是要建立兩者之間的聯繫模型，然後這個模型就可以應用在新的數據上進行標記。它可以進一步分為*分類*和*回歸*任務：在分類中，標記的是離散的分組，而在回歸中，標記的是連續的量。我們在後續章節中會看到這兩種有監督學習的例子。\n",
    "\n",
    "> *Unsupervised learning* involves modeling the features of a dataset without reference to any label, and is often described as \"letting the dataset speak for itself.\"\n",
    "These models include tasks such as *clustering* and *dimensionality reduction.*\n",
    "Clustering algorithms identify distinct groups of data, while dimensionality reduction algorithms search for more succinct representations of the data.\n",
    "We will see examples of both types of unsupervised learning in the following section.\n",
    "\n",
    "*無監督學習*是從沒有標記的數據中建立模型，它常被描述為“讓數據集自己說話”。這樣的模型包括*聚類*和*降維*。聚類算法能識別數據中的分組，而降維算法尋找數據更簡潔的表達形式。我們在後續章節中會看到這兩種無監督學習的例子。\n",
    "\n",
    "> In addition, there are so-called *semi-supervised learning* methods, which falls somewhere between supervised learning and unsupervised learning.\n",
    "Semi-supervised learning methods are often useful when only incomplete labels are available.\n",
    "\n",
    "除此之外，還有一種被成為*半監督學習*的方法，介於有監督學習和無監督學習之間。半監督學習方法經常應用在不完整的數據標記的場合中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification: Predicting discrete labels\n",
    "\n",
    "### 1.分類：預測離散的標籤\n",
    "\n",
    "> We will first take a look at a simple *classification* task, in which you are given a set of labeled points and want to use these to classify some unlabeled points. Imagine that we have the data shown in this figure:\n",
    "\n",
    "我們首先看一個簡單的分類任務，你有一組標記過的點，然後你使用這些數據來標記新的未標記過的數據點。我們有下圖展示的數據："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-classification-1.png)\n",
    "[附录中生成图像的代码](06.00-Figure-Code.ipynb#Classification-Example-Figure-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we have two-dimensional data: that is, we have two *features* for each point, represented by the *(x,y)* positions of the points on the plane.\n",
    "In addition, we have one of two *class labels* for each point, here represented by the colors of the points.\n",
    "From these features and labels, we would like to create a model that will let us decide whether a new point should be labeled \"blue\" or \"red.\"\n",
    "\n",
    "這裡我們每個點我們都有兩個*特徵* (2維)，使用平面中的*(x,y)*位置表示。除此之外，我們對每個點都有一個標記，標記一共有兩種，上圖中使用了顏色進行區分。使用這些特徵和標記，我們可以建立一個模型對新的數據點進行標記，判斷它屬於“藍色”還是“紅色”。\n",
    "\n",
    "> There are a number of possible models for such a classification task, but here we will use an extremely simple one. We will make the assumption that the two groups can be separated by drawing a straight line through the plane between them, such that points on each side of the line fall in the same group.\n",
    "Here the *model* is a quantitative version of the statement \"a straight line separates the classes\", while the *model parameters* are the particular numbers describing the location and orientation of that line for our data.\n",
    "The optimal values for these model parameters are learned from the data (this is the \"learning\" in machine learning), which is often called *training the model*.The following figure shows a visual representation of what the trained model looks like for this data:\n",
    "\n",
    "我們假設這兩組數據點可以使用一條平面上的直線進行區分，直線兩邊分別屬於兩個不同的組。這裡的*模型*是“一條分類直線”說法的定量版本，而*模型中的參數*就是用來描述直線位置和方向的特殊數字。優化後的模型參數值是從數據中學習得到的，這個學習過程我們通常成為*訓練模型*。\n",
    "\n",
    "> Now that this model has been trained, it can be generalized to new, unlabeled data.\n",
    "In other words, we can take a new set of data, draw this model line through it, and assign labels to the new points based on this model.\n",
    "This stage is usually called *prediction*. See the following figure:\n",
    "\n",
    "當模型訓練好之後，它就能泛化到新的未標記的數據上。換一種說法是，我們可以取一組新的數據，將模型的直線畫上去穿過它們，然後給新的數據點定義標籤。這個階段通常被稱為*預測*。參見下面的圖："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-classification-3.png)\n",
    "[附录中生成图像的代码](06.00-Figure-Code.ipynb#Classification-Example-Figure-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression: Predicting continuous labels\n",
    "\n",
    "### 2.回歸：預測連續標籤\n",
    "\n",
    "> In contrast with the discrete labels of a classification algorithm, we will next look at a simple *regression* task in which the labels are continuous quantities. Consider the data shown in the following figure, which consists of a set of points each with a continuous label:\n",
    "\n",
    "對比離散標籤分類算法，我們下面來看一個簡單的回歸，它的標籤是一個連續的數量。如下圖，包含著一組的數據點每一個都有一個連續的標籤："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-regression-1.png)\n",
    "[附录中生成图像的代码](06.00-Figure-Code.ipynb#Regression-Example-Figure-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As with the classification example, we have two-dimensional data: that is, there are two features describing each data point.The color of each point represents the continuous label for that point.\n",
    "\n",
    "就像分類例子中那樣，我們有著二維的數據：即每個數據點都有兩個特徵。每個點的顏色代表這這個點的連續標籤。\n",
    "\n",
    "> There are a number of possible regression models we might use for this type of data, but here we will use a simple linear regression to predict the points.This simple linear regression model assumes that if we treat the label as a third spatial dimension, we can fit a plane to the data.This is a higher-level generalization of the well-known problem of fitting a line to data with two coordinates.\n",
    "\n",
    "對於這個數據集來說，可以有很多種可能的回歸模型，但是這裡我們會使用一種簡單的線性回歸來預測數據點。這個簡單的線性回歸模型假設我們將數據標籤作為第三個空間維度，我們可以在上面使用一個平面來擬合數據。這是在兩個坐標中使用一根直線來擬合數據的泛化版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-regression-2.png)\n",
    "[附录中生成图像的代码](06.00-Figure-Code.ipynb#Regression-Example-Figure-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that the *feature 1-feature 2* plane here is the same as in the two-dimensional plot from before; in this case, however, we have represented the labels by both color and three-dimensional axis position.\n",
    "From this view, it seems reasonable that fitting a plane through this three-dimensional data would allow us to predict the expected label for any set of input parameters.\n",
    "Returning to the two-dimensional projection, when we fit such a plane we get the result shown in the following figure:\n",
    "\n",
    "注意上圖中的*特徵1 - 特徵2*平面與前面二維圖中數據點是一致的；我們使用了顏色以及三維坐標表示數據點的標籤。從上圖中我們可以看到，通過這個平面可以讓我們對任意輸入的數據點參數進行標籤的預測。返回到二維投射，當我們擬合了這個平面我們會得到下圖的結果："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-regression-3.png)\n",
    "[附录中生成图像的代码](06.00-Figure-Code.ipynb#Regression-Example-Figure-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This plane of fit gives us what we need to predict labels for new points.\n",
    "Visually, we find the results shown in the following figure:\n",
    "\n",
    "擬合得到的平面能為我們提供預測新數據點標籤的能力。下面的圖像展示了預測的結果："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-regression-4.png)\n",
    "[附录中生成图像的代码](06.00-Figure-Code.ipynb#Regression-Example-Figure-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering: Inferring labels on unlabeled data\n",
    "\n",
    "### 3.聚類：在未標記的數據上推斷標籤\n",
    "\n",
    "> The classification and regression illustrations we just looked at are examples of supervised learning algorithms, in which we are trying to build a model that will predict labels for new data.Unsupervised learning involves models that describe data without reference to any known labels.One common case of unsupervised learning is \"clustering,\" in which data is automatically assigned to some number of discrete groups.For example, we might have some two-dimensional data like that shown in the following figure:\n",
    "\n",
    "上面介紹的分類和回歸為我們展示了使用有監督學習算法的例子，我們會從數據中學習得到一個模型然後使用它預測新數據的標籤。無監督學習用來描述數據的模型是從沒有任何已知標籤的數據中獲得的。最常見的無監督學習場景是“聚類”，其中的數據自動組合成一些離散的分組。例如下圖中展示的二維數據："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-clustering-1.png)\n",
    "[附录中生成图像的代码](06.00-Figure-Code.ipynb#Clustering-Example-Figure-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By eye, it is clear that each of these points is part of a distinct group.\n",
    "Given this input, a clustering model will use the intrinsic structure of the data to determine which points are related.\n",
    "Using the very fast and intuitive *k*-means algorithm, we find the clusters shown in the following figure: *k*-means fits a model consisting of *k* cluster centers; the optimal centers are assumed to be those that minimize the distance of each point from its assigned center.Again, this might seem like a trivial exercise in two dimensions, but as our data becomes larger and more complex, such clustering algorithms can be employed to extract useful information from the dataset.\n",
    "\n",
    "\n",
    "肉眼觀察可以知道很顯然這些數據點是不同分組的組成部分。對於這個輸入來說，一個聚類模型會使用輸入數據的內在結構來找到哪些點是關聯的。使用下面快速直觀的*k均值*算法，我們會發現如下聚類：*k均值*會訓練出一個包括*k*個聚類中心點的模型；優化後的中心點應該是屬於這個聚類群的所有點距離之和最小的點。還是需要說明的是在二維的情況下，這看起來有點平淡無奇，但是當我們數據變得更大更複雜時，這種聚類算法可以用來從數據集中提取出有用的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-clustering-2.png)\n",
    "[附录中生成图像的代码](06.00-Figure-Code.ipynb#Clustering-Example-Figure-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction: Inferring structure of unlabeled data\n",
    "\n",
    "###  降維：推斷無標記數據的結構\n",
    "\n",
    "> Dimensionality reduction is another example of an unsupervised algorithm, in which labels or other information are inferred from the structure of the dataset itself.\n",
    "Dimensionality reduction is a bit more abstract than the examples we looked at before, but generally it seeks to pull out some low-dimensional representation of data that in some way preserves relevant qualities of the full dataset.\n",
    "Different dimensionality reduction routines measure these relevant qualities in different ways\n",
    "\n",
    "降維是另一個無監督算法的例子，它能從數據集本身的結構推斷標籤或其他的信息。降維通過用更少維度的數據表達但是卻保留了完整數據集的相關關鍵信息。不同的降維算法從不同方面衡量這些相關信息，使用下圖展示的數據作為例子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-dimesionality-1.png)\n",
    "[附录中产生图像的代码](06.00-Figure-Code.ipynb#Dimensionality-Reduction-Example-Figure-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Visually, it is clear that there is some structure in this data: it is drawn from a one-dimensional line that is arranged in a spiral within this two-dimensional space.\n",
    "In a sense, you could say that this data is \"intrinsically\" only one dimensional, though this one-dimensional data is embedded in higher-dimensional space.\n",
    "A suitable dimensionality reduction model in this case would be sensitive to this nonlinear embedded structure, and be able to pull out this lower-dimensionality representation. The following figure shows a visualization of the results of the Isomap algorithm, a manifold learning algorithm that does exactly this:\n",
    "\n",
    "從圖上很容易看出數據有一些內在的結構：數據是由一維的線捲曲成螺旋狀的二維形狀。或者直覺上你可以認為數據本質上是一維的，不過是嵌入在一個更高維度的空間中。一個合適的降維模型可以在這個情況下感知這種非線性的內嵌結構，並且能夠將其低維度的數據表現方式提取出來。下面展示了使用Isomap算法的可視化結果，這是一種適合該應用場景的流形學習算法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-dimesionality-2.png)\n",
    "[附录中生成图像的代码](06.00-Figure-Code.ipynb#Dimensionality-Reduction-Example-Figure-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that the colors (which represent the extracted one-dimensional latent variable) change uniformly along the spiral, which indicates that the algorithm did in fact detect the structure we saw by eye.\n",
    "As with the previous examples, the power of dimensionality reduction algorithms becomes clearer in higher-dimensional cases.\n",
    "For example, we might wish to visualize important relationships within a dataset that has 100 or 1,000 features.\n",
    "Visualizing 1,000-dimensional data is a challenge, and one way we can make this more manageable is to use a dimensionality reduction technique to reduce the data to two or three dimensions.\n",
    "\n",
    "注意到上圖中的顏色（代表著提取出來的一維隱變量）是沿著螺旋線均勻變化的，這表明算法確實能夠檢測到我們肉眼觀察到的結構。降維算法的威力同樣可以在更高維度的數據中更好的展現出來。例如，我們希望將具有100或1000個特徵的數據集的重要關聯關係在圖中可視化出來，可視化1000維度的數據是非常具有挑戰性的，我們可以通過降維技術將數據維度減少到二維或三維，這就很容易實現可視化了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "> Here we have seen a few simple examples of some of the basic types of machine learning approaches.\n",
    "Needless to say, there are a number of important practical details that we have glossed over, but I hope this section was enough to give you a basic idea of what types of problems machine learning approaches can solve.\n",
    "\n",
    "本節中我們看到了一些基本機器學習方法的簡單例子。無需說明也看得出來，我們只是一筆帶過的進行了相關介紹，但通過本節的內容希望能為讀者提供了關於機器學習方法能夠解決的問題類型的基本概念。\n",
    "\n",
    "*Supervised learning*: Models that can predict labels based on labeled training data\n",
    "- *Classification*: Models that predict labels as two or more discrete categories\n",
    "- *Regression*: Models that predict continuous labels\n",
    "  \n",
    "*Unsupervised learning*: Models that identify structure in unlabeled data\n",
    "- *Clustering*: Models that detect and identify distinct groups in the data\n",
    "- *Dimensionality reduction*: Models that detect and identify lower-dimensional structure in higher-dimensional data\n",
    "  \n",
    "*有監督學習*：建立一個能夠根據帶標記的訓練數據對數據進行標籤預測的模型\n",
    "- *分類*：建立一個能夠預測兩個或多個離散分組標籤的模型\n",
    "- *回歸*：建立一個能夠預測連續標籤的模型\n",
    "\n",
    "*無監督學習*：建立一個能夠識別未標記數據內在結構的模型\n",
    "- *聚類*：建立一個檢查和識別數據不同分組的模型\n",
    "- *降維*：建立一個能發現高維度數據在低維度情況下結構的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [机器学习](05.00-Machine-Learning.ipynb) | [目录](Index.ipynb) | [Scikit-Learn简介](05.02-Introducing-Scikit-Learn.ipynb) >\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/wangyingsm/Python-Data-Science-Handbook/blob/master/notebooks/05.01-What-Is-Machine-Learning.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
