{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "160ba181",
   "metadata": {},
   "source": [
    "##### Information from : https://ithelp.ithome.com.tw/articles/10261285"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e9a37c",
   "metadata": {},
   "source": [
    "## NLP Web\n",
    "\n",
    "為了運用正則表達式來製造pattern，我們先引入模組 re 。這時候我們使用 re.sub() 這個函式，並且傳遞三個必要引數(required arguments)：\n",
    "- pattern: 正則表達式，在這裡我們可以設計為 r\"<.*?>\"\n",
    "- replacement_text: 符合pattern的字串將被更換為之，在這裡直接換成空字串 ''\n",
    "- input: 待比對之字串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b2e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize  #NLTK工具箱 sent_tokenize() 用來實現斷句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1dd009b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "   \n",
      "      My Garden - Tomatoes\n",
      "   \n",
      "   \n",
      "   Garden Tomatoes\n",
      "   I decided to plant some tomatoes this spring. They're really taking off and I hope to have lots of tomatoes to give to all my friends and family this summer!\n",
      "   Here are a few things I like about tomatoes:\n",
      "   \n",
      "      They taste great.\n",
      "      They're good for me.\n",
      "      They're easy to grow!\n",
      "   \n",
      "   Here's a picture of my garden:\n",
      "   \n",
      "   Here's a link to check out more interesting things about tomatoes!\n",
      "   \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"\"\"\n",
    "<html>\n",
    "   <head>\n",
    "      <title>My Garden - Tomatoes</title>\n",
    "   </head>\n",
    "   <body>\n",
    "   <h1>Garden Tomatoes</h1>\n",
    "   <p>I decided to plant some tomatoes this spring. They're really taking off and I hope to have lots of tomatoes to give to all my friends and family this summer!</p>\n",
    "   <p>Here are a few things I like about tomatoes:</p>\n",
    "   <ol>\n",
    "      <li>They taste great.</li>\n",
    "      <li>They're good for me.</li>\n",
    "      <li>They're easy to grow!</li>\n",
    "   </ol>\n",
    "   <p>Here's a picture of my garden:</p>\n",
    "   <img src=\"http://www.mygardensite.com/images/my-garden-001.jpg\" alt=\"a picture of my garden\" />\n",
    "   <p>Here's a <a href=\"http://www.welovetomatoes.com\">link</a> to check out more interesting things about tomatoes!</p>\n",
    "   </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "text_no_tags = re.sub(r\"<.*?>\", '', raw_text)\n",
    "print(text_no_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c27b32c",
   "metadata": {},
   "source": [
    "## Cleaning the black\n",
    "使用代表 whitspace、tab、換行的元字元(metacharacter) \\s。由於無意義空格佔了兩個半格以上的空間，因此pattern可以設計為 \\s{2,}，程式碼如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b42f8fbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to remove redundant whitespaces\n",
    "text_no_whitespace = re.sub(r\"\\s{2,}\", ' ', text_no_tags)\n",
    "text_no_whitespace\n",
    "type(text_no_whitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da55cab",
   "metadata": {},
   "source": [
    "## Sentence Segmenation 斷句\n",
    "\n",
    "在 Python 的實踐上，我們使用自然語言處理工具箱 NLTK (NLP Toolkit) 來協助我們進行處理任務。第一步我們欲將以上的字串拆分成多個句子，此步驟稱之為斷句（Sentence Segmentation）。句號（.）是判斷句子結束很好的依據，但仍有些例外－省略用的句號，如 Mr. Williams、 Ph.D. 等。 好消息是， NLTK工具箱當中的 tokenize 模組，已經定義好了函式 sent_tokenize() 用來實現斷句："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c905a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Sentence 1: Facebook under fire over secret teen research\n",
      "By Jane Wakefield\n",
      "Technology reporter Published15 September 2021\n",
      "Girl taking a selfie\n",
      "IMAGE SOURCE,GETTY IMAGES\n",
      "Image caption,\n",
      "Teenage girls can be very conscious of body image - and Instagram can make them feel worse, the internal studies showed\n",
      "Facebook-owned Instagram has been criticised for keeping secret its internal research into the effect social media had on teenager users.\n",
      "\n",
      "Sentence 2: According to the Wall Street Journal, its studies showed teenagers blamed Instagram for increased levels of anxiety and depression.\n",
      "\n",
      "Sentence 3: Campaign groups and MPs have said it is proof the company puts profit first.\n",
      "\n",
      "Sentence 4: Instagram said the research showed its commitment to understanding complex and difficult issues.\n",
      "\n",
      "Sentence 5: The Wall Street Journal's report, not disputed by Facebook, finds: A 2019 presentation slide said: We make body-image issues worse for one in three teenage girls\n",
      "Another slide said teenagers blamed Instagram for increased levels of anxiety and depression\n",
      "In 2020, research found 32% of teenage girls surveyed said when they felt bad about their bodies, Instagram made them feel worse\n",
      "Some 13% of UK teenagers and 6% of US users surveyed traced a desire to kill themselves to Instagram\n",
      "Instagram conducted multiple focus groups, online surveys and diary studies over a number of years\n",
      "In 2021, it conducted large-scale research of tens of thousands of people that paired user responses with its own data about time spent on Instagram and what was viewed\n",
      "In response to the WSJ report, Instagram published a lengthy blog defending its research.\n",
      "\n",
      "Sentence 6: The WSJ story focused on a limited set of findings and casts them in a negative light, it said, but the issue was far more complex.\n",
      "\n",
      "Sentence 7: Two teens taking a selfie\n",
      "IMAGE SOURCE,GETTY IMAGES\n",
      "Image caption,\n",
      "According to Facebook, people's relationship with social media is complex - it makes them feel both good and bad\n",
      "We've done extensive work around bullying, suicide and self-injury, and eating disorders, to help make Instagram a safe and supportive place for everyone, the company said in its post.\n",
      "\n",
      "Sentence 8: Based on our research and feedback from experts, we've developed features so people can protect themselves from bullying, we've given everyone the option to hide 'like' counts and we've continued to connect people who may be struggling with local support organisations.\n",
      "\n",
      "Sentence 9: It was working on prompts to encourage people repeatedly dwelling on negative subjects to look at different topics, it said.\n",
      "\n",
      "Sentence 10: And it promised to be more transparent about its research in future.\n",
      "\n",
      "Sentence 11: 'Profit before harm'\n",
      "But National Society for the Prevention of Cruelty to Children head of child safety online Andy Burrows said it was appalling they chose to sit on their hands rather than act on evidence.\n",
      "\n",
      "Sentence 12: Instead of working to make the site safe, they've obstructed researchers, regulators, and government and run a PR [public-relations] and lobbying campaign in an attempt to prove the opposite.\n",
      "\n",
      "Sentence 13: MP Damian Collins, who is chairing the UK parliamentary committee looking at how big technology should be regulated to protect users' safety, said it was time to hold them to account.\n",
      "\n",
      "Sentence 14: The Wall Street Journal Facebook files investigation has exposed how the company, time and again, puts profit before harm, he said.\n",
      "\n",
      "Sentence 15: Its own research is telling it that a large number of teen Instagram users say the service makes them feel worse about themselves - but the company just wants to make sure they keep coming back.\n",
      "\n",
      "Sentence 16: The Online Safety Bill aims to give regulator Ofcom the power to fine companies that fail to act on content that could cause harm.\n",
      "\n",
      "Sentence 17: Young girl using a filter\n",
      "IMAGE SOURCE,GETTY IMAGES\n",
      "Image caption,\n",
      "Instagram is popular with young children, despite a joining age of 13\n",
      "US campaign group Fairplay (formerly the Campaign for a Commercial-Free Childhood) said the news showed Instagram was no place for children.\n",
      "\n",
      "Sentence 18: In a move straight out of big tobacco's playbook, Facebook downplayed the negative effects of its product and hid this research from the public and even from members of Congress who specifically asked for it, it said.\n",
      "\n",
      "Sentence 19: And in the ultimate display of chutzpah and disregard for children, the company now wants to hook young kids on Instagram.\n",
      "\n",
      "Sentence 20: Fairplay also called on the US government to demand Facebook released its research and blocked its plans to launch Instagram Youth.\n",
      "\n",
      "Sentence 21: It was revealed earlier this year Facebook planned to create an advert-free Instagram for under-13s, designed to keep them safe.\n",
      "\n",
      "Sentence 22: 'No fix'\n",
      "Jonathan Haidt, a social psychologist at New York University's Stern School of Business, told BBC Radio 4's Today programme he had met Facebook boss Mark Zuckerberg to discuss the social network's effect on mental health.\n",
      "\n",
      "Sentence 23: He was interested but he believes the research is ambiguous and does not point to harm, Mr Haidt said.\n",
      "\n",
      "Sentence 24: Of course, now we know they had their own research which did suggest harm.\n",
      "\n",
      "Sentence 25: They had focus groups, online surveys, diary studies - so this was not one chance finding.\n",
      "\n",
      "Sentence 26: I wouldn't expect them to come forward the first time they find evidence of harm and say, 'Oh my God our product is harmful,' but if they have multiple sources of evidence and there is evidence outside the company too, then I think the picture is pretty clear.\n",
      "\n",
      "Sentence 27: But, he added, it would take root-and-branch changes at the company to make any difference.\n",
      "\n",
      "Sentence 28: The platform encourages children to post photos of themselves, to be raided by others including strangers around the world, he said.\n",
      "\n",
      "Sentence 29: If this is the business model, there is no way to fix it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# removing double quotes\n",
    "text = re.sub(r\"\\\"\", '', text_no_whitespace)\n",
    "\n",
    "# breaking text into sentences\n",
    "text_sentences = sent_tokenize(text)\n",
    "print(type(text_sentences))\n",
    "\n",
    "# printing out sentences\n",
    "for i, sent in enumerate(text_sentences):\n",
    "    print(\"Sentence {}: {}\".format(i + 1, sent), end = \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc320fa",
   "metadata": {},
   "source": [
    "## Tokenisation 斷詞\n",
    "進一步將句子拆分成更小的單位－單詞。值得注意的是，在英文當中單詞通常被認為能夠表示意義的最小單位－詞條（Token），將字串拆分成詞條的過程就是斷詞（word segementation），又稱記號化（Tokenisation）。此詞我們引入另一個拆分函式 word_tokenize() ："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f3e518",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:  My Garden - Tomatoes Garden Tomatoes I decided to plant some tomatoes this spring.\n",
      "['My', 'Garden', '-', 'Tomatoes', 'Garden', 'Tomatoes', 'I', 'decided', 'to', 'plant', 'some', 'tomatoes', 'this', 'spring', '.']\n",
      "\n",
      "Sentence 2: They're really taking off and I hope to have lots of tomatoes to give to all my friends and family this summer!\n",
      "['They', \"'re\", 'really', 'taking', 'off', 'and', 'I', 'hope', 'to', 'have', 'lots', 'of', 'tomatoes', 'to', 'give', 'to', 'all', 'my', 'friends', 'and', 'family', 'this', 'summer', '!']\n",
      "\n",
      "Sentence 3: Here are a few things I like about tomatoes: They taste great.\n",
      "['Here', 'are', 'a', 'few', 'things', 'I', 'like', 'about', 'tomatoes', ':', 'They', 'taste', 'great', '.']\n",
      "\n",
      "Sentence 4: They're good for me.\n",
      "['They', \"'re\", 'good', 'for', 'me', '.']\n",
      "\n",
      "Sentence 5: They're easy to grow!\n",
      "['They', \"'re\", 'easy', 'to', 'grow', '!']\n",
      "\n",
      "Sentence 6: Here's a picture of my garden: Here's a link to check out more interesting things about tomatoes!\n",
      "['Here', \"'s\", 'a', 'picture', 'of', 'my', 'garden', ':', 'Here', \"'s\", 'a', 'link', 'to', 'check', 'out', 'more', 'interesting', 'things', 'about', 'tomatoes', '!']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#List\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "for i, sent in enumerate(text_sentences):\n",
    "    print(\"Sentence {}: {}\".format(i + 1, sent))\n",
    "    tokens = word_tokenize(sent)\n",
    "    print(tokens, end = \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9de8f067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The First sentence is about Python.', 'The Second: about Django.', 'You can learn Python, Django and Data Ananlysis here.']\n"
     ]
    }
   ],
   "source": [
    "#String\n",
    "sentence_data = \"The First sentence is about Python. The Second: about Django. You can learn Python, \\\n",
    "Django and Data Ananlysis here. \"\n",
    "\n",
    "nltk_tokens = nltk.sent_tokenize(sentence_data)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcaa427",
   "metadata": {},
   "source": [
    "### token.lower() 大小寫轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f0d94d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'spectators',\n",
       " 'all',\n",
       " 'stood',\n",
       " 'and',\n",
       " 'sang',\n",
       " 'the',\n",
       " 'national',\n",
       " 'anthem']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised = [\"The\", \"spectators\", \"all\", \"stood\", \"and\", \"sang\", \"the\", \"national\", \"anthem\"]\n",
    "# lowercasing each token\n",
    "tokens_lower = [token.lower() for token in tokenised] \n",
    "tokens_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2572a5e",
   "metadata": {},
   "source": [
    "## Stemming 語幹提取\n",
    "在語言學中，詞幹（word stem）表示一個單詞中最基本且核心的形式，例如 friendships 就是由 friendship 與詞綴 -s 所組成， friendship 就是其詞幹；而 friendship 則是由 friend 與詞綴 -ship 所構成，此時 friend 則是其詞幹。因此詞幹的提取基於不同理念或不同演算法，有時會得到不同的結果。我們以常見的 Porter Stemming Algorithm、 Lancaster Stemming Algorithm 以及 Snowball Stemming Algorithm 說明，從而比較它們的差異。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1add59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter: ['the', 'spectat', 'all', 'stood', 'and', 'sang', 'the', 'nation', 'anthem']\n",
      "Lancaster: ['the', 'spect', 'al', 'stood', 'and', 'sang', 'the', 'nat', 'anthem']\n",
      "Snowball: ['the', 'spectat', 'all', 'stood', 'and', 'sang', 'the', 'nation', 'anthem']\n"
     ]
    }
   ],
   "source": [
    "# importing stemmer classes\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "tokens = [\"the\", \"spectators\", \"all\", \"stood\", \"and\", \"sang\", \"the\", \"national\", \"anthem\"]\n",
    "\n",
    "# stemming\n",
    "port = PorterStemmer()\n",
    "stemmed_port = [port.stem(token) for token in tokens]\n",
    "\n",
    "lanc = LancasterStemmer()\n",
    "stemmed_lanc = [lanc.stem(token) for token in tokens]\n",
    "\n",
    "snow = SnowballStemmer(\"english\")\n",
    "stemmed_snow = [snow.stem(token) for token in tokens]\n",
    "\n",
    "# showing stemmed results\n",
    "print(\"Porter: {}\".format(stemmed_port)) \n",
    "print(\"Lancaster: {}\".format(stemmed_lanc))\n",
    "print(\"Snowball: {}\".format(stemmed_snow))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1ef83",
   "metadata": {},
   "source": [
    "## 詞形還原（Lemmatisation）\n",
    "很顯然，萃取詞幹並未能滿足我們減少詞形變化（inflection）的需求，因此我們轉而找尋更能代表單詞基本形式－詞位（lemma），例如 sings、 singing、 sang、 sung 共享同一個詞位 sing。以下我們將借用 NLTK.stem 模組中收錄的 WordNetLemmatizer 類別找出詞位，WordNet為普林斯頓大學所建立的免費公開詞彙資料庫。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e24ff36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jerrychien/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1361c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatised: ['the', 'spectator', 'all', 'stood', 'and', 'sang', 'the', 'national', 'anthem']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokens = [\"the\", \"spectators\", \"all\", \"stood\", \"and\", \"sang\", \"the\", \"national\", \"anthem\"]\n",
    "\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "lemmatised = [lemmatiser.lemmatize(token) for token in tokens]\n",
    "print(\"lemmatised: {}\".format(lemmatised))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794960b7",
   "metadata": {},
   "source": [
    "## 停用詞去除（Stopword Removal）\n",
    "在文句中有些單詞並對於詞義的傳達並無太大的作用，如 a/ an、 the 、 is/ are等，被稱之為停用詞（ stop words）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d933cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words removed: ['spectator', 'stood', 'sang', 'national', 'anthem']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jerrychien/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# defining stopwords in English\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# removing stop words\n",
    "words_no_stop = [word for word in lemmatised if word not in stop_words]\n",
    "print(\"stop words removed: {}\".format(words_no_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb63364",
   "metadata": {},
   "source": [
    "## 詞性標註（POS Tagging）\n",
    "詞性（Part-of-Speech, POS）與語法分析（Syntactic Analysis）\n",
    "在語言學中，單詞被依照其功能以及詞形變化（inflection）分類為不同的詞性（Part of Speech, POS）。常見的詞性包含了名詞、動詞、形容詞、副詞、介係詞等等，如「 In God we trust. 」這句英文就由介係詞（ in ） + 名詞（ God ） + 代名詞（ we ）+ 動詞（ trust ） 所依序構成，其句法（syntax）有別於由代名詞、動詞、介係詞、名詞依序構成的「 We trust in God. 」。我們將以詞性作為出發點，依循文法規則，進而分析文句的架構，這個過程稱為語法分析（syntactic analysis）。\n",
    "- https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0998a754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagged sentence:\n",
      "[('their', 'PRP$'), ('decision', 'NN'), ('makes', 'VBZ'), ('no', 'DT'), ('economic', 'JJ'), ('sense', 'NN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jerrychien/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "tokenised_sent = [\"their\", \"decision\", \"makes\", \"no\", \"economic\", \"sense\"]\n",
    "\n",
    "# POS tagging\n",
    "pos_tagged_sent = pos_tag(tokenised_sent)\n",
    "print(\"POS tagged sentence:\\n{}\".format(pos_tagged_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dddb4a",
   "metadata": {},
   "source": [
    "## 語義組塊（Phrase Chunking）\n",
    "句構的層次描述可以很簡單，也可以很複雜，取決於我們如何「分塊（ chunking ）」。我們可以依照片語或子句的文法結構指定語義組塊，透過語法剖析器（ parser ）逐步檢查語法（使用正則表達式比對字串），從而產生描述層次結構的分析樹。我們將示範以名詞片語以及動詞片語兩個簡單的文法結構來實踐分塊："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6f32896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ee1e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 名詞片語 given a word tokenised sentence\n",
    "tokenised_sent = [\"their\", \"decision\", \"makes\", \"no\", \"economic\", \"sense\"]\n",
    "\n",
    "# POS tagging\n",
    "pos_tagged_sent = pos_tag(tokenised_sent)\n",
    "\n",
    "# specifying the formal grammar of an noun phrase: \"grammar_name: {RegEx}\"\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN.?>}\"\n",
    "# building its parser\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "# chunk parsing a sentence\n",
    "np_chunked_sent = np_chunk_parser.parse(pos_tagged_sent)\n",
    "\n",
    "# visualising parsing result\n",
    "np_chunked_sent.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb356c",
   "metadata": {},
   "source": [
    "## 應用實例：文章資訊檢索\n",
    "介紹完了文法解析之後，我們接下來瀏覽一篇新聞報導，藉由一系列前處理、詞性標籤以及語塊分析的技巧，找出文本中的關鍵資訊。\n",
    "我們預先寫好兩個模組：tokenise_words.py 以及 chunk_counters.py\n",
    "以下為 tokenise_words 模組：將清理過的字串進行斷句與斷詞（小寫轉換 → 斷句 → 斷詞）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641de9e",
   "metadata": {},
   "source": [
    "## 詞袋模型（Bag-of-Words Model, BoW）\n",
    "淺談詞「袋」\n",
    "詞袋模型是一個基於單詞出現頻率來表示文字的方法，它並不考慮單詞的排列順序、或甚至是文法結構。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ffd14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
