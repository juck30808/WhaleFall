{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from ipywidgets import interact_manual\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 7)\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'jobs.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9q/486czkcn7lv5v0hwbt71twdc0000gn/T/ipykernel_10917/619369119.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# lets import the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'jobs.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'jobs.csv'"
     ]
    }
   ],
   "source": [
    "# lets import the dataset\n",
    "jobs = pd.read_csv('jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the shapes of the datasets used \n",
    "print(\"Shape of Jobs Data :\", jobs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the null values in the data\n",
    "print(\"Null values in Jobs Data :\", jobs.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the head of the jobs data\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 4, 1)\n",
    "sns.countplot(jobs['FinancialBudget'], palette = 'pink')\n",
    "plt.ylabel(\" \")\n",
    "\n",
    "plt.subplot(2, 4, 2)\n",
    "sns.countplot(jobs['ContactLevel'], palette = 'bone')\n",
    "plt.ylabel(\" \")\n",
    "\n",
    "plt.subplot(2, 4, 3)\n",
    "sns.countplot(jobs['OrgImpact'], palette = 'Reds')\n",
    "plt.ylabel(\" \")\n",
    "\n",
    "plt.subplot(2, 4, 4)\n",
    "sns.countplot(jobs['Supervision'], palette = 'summer')\n",
    "plt.ylabel(\" \")\n",
    "\n",
    "plt.subplot(2, 4, 5)\n",
    "sns.countplot(jobs['ProblemSolving'], palette = 'winter')\n",
    "plt.ylabel(\" \")\n",
    "\n",
    "plt.subplot(2, 4, 6)\n",
    "sns.countplot(jobs['Experience'], palette = 'Blues')\n",
    "plt.ylabel(\" \")\n",
    "\n",
    "plt.subplot(2, 4, 7)\n",
    "sns.countplot(jobs['EducationLevel'])\n",
    "plt.ylabel(\" \")\n",
    "\n",
    "plt.subplot(2, 4, 8)\n",
    "sns.countplot(jobs['PayGrade'])\n",
    "plt.ylabel(\" \")\n",
    "\n",
    "plt.suptitle('Job Vacancies for different Levels',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the correlation map\n",
    "\n",
    "sns.heatmap(jobs[['PayGrade','EducationLevel',\n",
    "                  'Experience','OrgImpact','ProblemSolving',\n",
    "                  'Supervision','ContactLevel','FinancialBudget']].corr(),\n",
    "            cmap = 'Greens',\n",
    "            annot = True)\n",
    "plt.title('Correlation Map for the Jobs Data', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets group the data wrt jobs family\n",
    "\n",
    "x = jobs.groupby(['JobFamilyDescription']).agg('mean')\n",
    "x = x.drop(['ID','JobFamily','JobClass'], axis = 1)\n",
    "x.style.background_gradient(cmap = 'Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets compare all attributes at once\n",
    "plt.rcParams['figure.figsize'] = (15, 6)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "x.plot()\n",
    "plt.xticks(np.arange(15), ('Accounting And Finance', 'Administrative Support', 'Baker',\n",
    "       'Buildings And Facilities', 'Buyer', 'Cashier',\n",
    "       'Communications And Media', 'Corporate Research',\n",
    "       'Finance  And Accounting', 'Human Resources', 'Meat Cutter', 'Produce',\n",
    "       'Secretary', 'Stockkeeping', 'Systems Analyst'),\n",
    "        rotation = 90)\n",
    "plt.title('Comparison of Different Jobs', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lets make an Interactive visualization\n",
    "\n",
    "@interact_manual\n",
    "def check(column = jobs.select_dtypes('number').columns[3:],\n",
    "          column1 = jobs.select_dtypes('number').columns[4:],):\n",
    "    sns.barplot(jobs[column], jobs[column1])\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lets check the head of naukri\n",
    "data = pd.read_csv('naukri.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the shape of the dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the head of the dataset\n",
    "data.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Job Location Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['joblocation_adress'] = data['joblocation_address'].str.split(',')\n",
    "pd.set_option('max_rows', 300000)\n",
    "data['joblocation_adress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.explode('joblocation_adress')\n",
    "pd.set_option('max_rows', 30000)\n",
    "data['joblocation_adress'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('(Bengaluru/Bangalore)',' Bangalore',\n",
    "                                                                ' Bangalore ','Bangalore ','Bengaluru/Bangalore ',\n",
    "                                                                 ' Bengaluru/Bangalore ',' Bengaluru/Bangalore',\n",
    "                                                                'Bengaluru/Bangalore','Bengaluru','NCR Bangalore',\n",
    "                                                                'NCR Bangalore ','Near Bangalore', ' Bengaluru/Bangalore',\n",
    "                                                                 ' Bengaluru/Bangalore ', ' Bengaluru',\n",
    "                                                                 'Bangalore , Bangalore / Bangalore',\n",
    "                                                                 'Bangalore , karnataka',' Bengaluru / Bangalore', \n",
    "                                                                 ' Bengaluru / Bangalore ', 'Bengaluru Bangalore',\n",
    "                                                                 'India-Karnataka-Bangalore',' bangalore',' karnataka',\n",
    "                                                                ' Bengaluru Bangalore'),\n",
    "                                                                ('Bangalore','Bangalore','Bangalore','Bangalore','Bangalore',\n",
    "                                                                'Bangalore','Bangalore','Bangalore','Bangalore','Bangalore',\n",
    "                                                                'Bangalore','Bangalore','Bangalore','Bangalore','Bangalore',\n",
    "                                                                 'Bangalore','Bangalore', 'Bangalore','Bangalore','Bangalore',\n",
    "                                                                'Bangalore','Bangalore','Bangalore','Bangalore',))\n",
    "                                                                \n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('Hyderabad / Secunderabad',' Hyderabad / Secunderabad',\n",
    "                                                                 ' Hyderabad / Secunderabad ','Hyderabad / Secunderabad ',\n",
    "                                                                ' Hyderabad','Hyderabad ',' Hyderabad ',\n",
    "                                                                 'Hyderabad/Secunderabad','Hyderabad/Secunderabad ',\n",
    "                                                                ' Hyderabad/Secunderabad ',' Hyderabad/Secunderabad',),\n",
    "                                                                ('Hyderabad', 'Hyderabad','Hyderabad','Hyderabad',\n",
    "                                                                 'Hyderabad', 'Hyderabad','Hyderabad','Hyderabad',\n",
    "                                                                'Hyderabad','Hyderabad','Hyderabad'))\n",
    "\n",
    "\n",
    "  \n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('NAVI MUMBAI',' NAVI MUMBAI','NAVI MUMBAI ',\n",
    "                                                                 ' NAVI MUMBAI',' NAVI MUMBAI ','Mumbai , Mumbai',\n",
    "                                                                 ' Mumbai',' Mumbai ','Mumbai ','mumbai','Navi Mumbai',\n",
    "                                                                ' Navi Mumbai',' Navi Mumbai ','Navi Mumbai ', \n",
    "                                                                 ' Mumbai Suburbs','Mumbai Suburbs ','Mumbai Suburbs',\n",
    "                                                                ' Mumbai Suburbs ','mumbai',' mumbai','mumbai ',\n",
    "                                                                 ' maharashtra'),\n",
    "                                                                ('Mumbai','Mumbai','Mumbai','Mumbai','Mumbai','Mumbai',\n",
    "                                                                 'Mumbai','Mumbai','Mumbai', 'Mumbai','Mumbai','Mumbai',\n",
    "                                                                'Mumbai','Mumbai','Mumbai','Mumbai','Mumbai','Mumbai','Mumbai',\n",
    "                                                                'Mumbai','Mumbai', 'Mumbai'))\n",
    "\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('Noida','Noida ',' Noida',' Delhi','Delhi','Delhi ',' Delhi ',\n",
    "                                                                 'Gurgaon',' Gurgaon',' Gurgaon ','Gurgaon ', ' noida',\n",
    "                                                                 ' Noida/Greater Noida',' Noida ', ' Delhi NCR',\n",
    "                                                                 'Delhi/NCR(National Capital Region)',' Delhi/NCR ',\n",
    "                                                                 ' Delhi/NCR(National Capital Region)',\n",
    "                                                                 ' Delhi/NCR(National Capital Region) ',\n",
    "                                                                 'Delhi/NCR(National Capital Region) ','Delhi , Delhi',\n",
    "                                                                 'Noida , Noida/Greater Noida','Ghaziabad',\n",
    "                                                                 'Delhi/NCR(National Capital Region) , Gurgaon',\n",
    "                                                                 'NCR , NCR','NCR/NCR(National Capital Region)',\n",
    "                                                                'NCR , NCR/Greater NCR','NCR/NCR(National Capital Region), NCR',\n",
    "                                                                 'NCR , NCR/NCR(National Capital Region)',\n",
    "                                                                 'NCR/NCR(National Capital Region)','NCR/Greater NCR',\n",
    "                                                                 'NCR/NCR(National Capital Region) , NCR','Delhi/NCR ',\n",
    "                                                                ' Noida/Greater Noida','Greater Noida',' Greater Noida',\n",
    "                                                                 ' Greater Noida ','Greater Noida ','Ghaziabad',' Ghaziabad',\n",
    "                                                                 'Ghaziabad ',' Ghaziabad ','Faridabad','Faridabad ',\n",
    "                                                                 ' Faridabad',' Faridabad ',' Noida/Greater Noida',\n",
    "                                                                 ' Noida/Greater Noida ',' delhi',' Delhi/NCR','Delhi NCR'\n",
    "                                                                ),\n",
    "                                                                ('NCR','NCR','NCR','NCR','NCR','NCR','NCR','NCR','NCR','NCR',\n",
    "                                                                'NCR','NCR','NCR','NCR','NCR','NCR','NCR','NCR','NCR','NCR',\n",
    "                                                                'NCR','NCR','NCR','NCR','NCR','NCR', 'NCR','NCR','NCR','NCR',\n",
    "                                                                 'NCR','NCR','NCR','NCR','NCR','NCR','NCR','NCR','NCR','NCR',\n",
    "                                                                 'NCR','NCR','NCR','NCR','NCR','NCR','NCR','NCR','NCR','NCR',\n",
    "                                                                'NCR'))\n",
    "\n",
    "\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('Chennai ',' Chennai',' Chennai ',' Chennai',\n",
    "                                                                 'chennai ',' chennai',' chennai ',' chennai',),\n",
    "                                                                ('Chennai', 'Chennai','Chennai','Chennai','Chennai',\n",
    "                                                                'Chennai','Chennai','Chennai',))\n",
    "\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('Pune ',' Pune',' Pune '),('Pune','Pune','Pune'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('Kolkata ',' Kolkata',' Kolkata ',\n",
    "                                                                ' kolkata','kolkata ',' kolkata'),\n",
    "                                                                ('Kolkata','Kolkata','Kolkata',\n",
    "                                                                  'Kolkata','Kolkata','Kolkata'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace((' Ahmedabad','Ahmedabad ',' Ahmedabad '),('Ahmedabad',\n",
    "                                                                                         'Ahmedabad','Ahmedabad'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('Chandigarh ',' Chandigarh',' Chandigarh '),\n",
    "                                                                ('Chandigarh','Chandigarh','Chandigarh'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('Surat ',' Surat',' Surat '),\n",
    "                                                                ('Surat','Surat','Surat'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('Ernakulam / Kochi/ Cochin ', ' Kochi', 'Kochi ',' Kochi ',\n",
    "                                                                 ' Cochin/ Kochi/ Ernakulam', ' Cochin/ Kochi/ Ernakulam ', \n",
    "                                                                 ' Ernakulam / Kochi/ Cochin',' Ernakulam / Kochi/ Cochin '),\n",
    "                                                                ('Kochi','Kochi','Kochi','Kochi','Kochi','Kochi','Kochi',\n",
    "                                                                 'Kochi',))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('Coimbatore ',' Coimbatore',' Coimbatore '),\n",
    "                                                                ('Coimbatore','Coimbatore','Coimbatore'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('Coimbatore ',' Coimbatore',' Coimbatore '),\n",
    "                                                                ('Coimbatore','Coimbatore','Coimbatore'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('Lucknow ',' Lucknow',' Lucknow '),\n",
    "                                                                ('Lucknow','Lucknow','Lucknow'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('Jaipur ',' Jaipur',' Jaipur ','jaipur ',' jaipur',\n",
    "                                                                 ' jaipur '),\n",
    "                                                                ('Jaipur','Jaipur','Jaipur','Jaipur','Jaipur','Jaipur'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('Vijayawada ',' Vijayawada',' Vijayawada '),\n",
    "                                                                ('Vijayawada','Vijayawada','Vijayawada'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace(('Visakhapatnam ',' Visakhapatnam',' Visakhapatnam ',\n",
    "                                                                'Visakhapatnam/Vizag ',' Visakhapatnam/Vizag',\n",
    "                                                                 ' Visakhapatnam/Vizag '),\n",
    "                                                                ('Visakhapatnam','Visakhapatnam','Visakhapatnam',\n",
    "                                                                 'Visakhapatnam','Visakhapatnam','Visakhapatnam',\n",
    "                                                                ))\n",
    "\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace((' Bhubaneshwar',' Bhubaneshwar',' Bhubaneshwar '),\n",
    "                                                                ('Bhubaneshwar','Bhubaneshwar','Bhubaneshwar'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace((' Patna',' Patna',' Patna '),\n",
    "                                                                ('Patna','Patna','Patna'))\n",
    "\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace((' Trivandrum',' Trivandrum',' Trivandrum '),\n",
    "                                                                ('Trivandrum','Trivandrum','Trivandrum'))\n",
    "\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace((' Mangalore',' Mangalore',' Mangalore '),\n",
    "                                                                ('Mangalore','Mangalore','Mangalore'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace((' Indore',' Indore',' Indore '),\n",
    "                                                                ('Indore','Indore','Indore'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace((' Guwahati',' Guwahati',' Guwahati '),\n",
    "                                                                ('Guwahati','Guwahati','Guwahati'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace((' Nagpur',' Nagpur',' Nagpur '),\n",
    "                                                                ('Nagpur','Nagpur','Nagpur'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace((' Raipur',' Raipur',' Raipur '),\n",
    "                                                                ('Raipur','Raipur','Raipur'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace((' Thane',' Thane',' Thane '),\n",
    "                                                                ('Thane','Thane','Thane'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace((' Bhopal',' Bhopal',' Bhopal '),\n",
    "                                                                ('Bhopal','Bhopal','Bhopal'))\n",
    "data['joblocation_adress'] = data['joblocation_adress'].replace((' Vadodara/Baroda',' Vadodara/Baroda',' Vadodara/Baroda ',\n",
    "                                                                ' Vadodara','Vadodara ',' Vadodara '),\n",
    "                                                                ('Vadodara','Vadodara','Vadodara','Vadodara',\n",
    "                                                                 'Vadodara','Vadodara',))\n",
    "                                                                                                                                  \n",
    "pd.set_option('max_rows', 2400)\n",
    "data['joblocation_adress'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data['joblocation_adress'].value_counts()\n",
    "data = data.loc[data['joblocation_adress'].isin(counts.index[counts > 25])]\n",
    "display(data['joblocation_adress'].value_counts())\n",
    "\n",
    "sns.countplot(data['joblocation_adress'], palette = 'inferno')\n",
    "plt.title('Locations with Highest Jobs', fontsize = 20)\n",
    "plt.xlabel(' ')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Experience Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['experience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 40000)\n",
    "data['experience'].str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['experience'] = data['experience'].str.split(\" \")\n",
    "data['Min Experience'] = data['experience'].apply(lambda x: x[0])\n",
    "data['Max Experience'] = data['experience'].apply(lambda x: x[2] if len(x) > 2 else x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Min Experience'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Max Experience'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Min Experience'] = data['Min Experience'].replace('Not',0)\n",
    "data['Max Experience'] = data['Max Experience'].replace(('Mentioned','-1'), (5,5))\n",
    "\n",
    "# lets convert thenm into numerical data types\n",
    "data['Min Experience'] = data['Min Experience'].astype('int')\n",
    "data['Max Experience'] = data['Max Experience'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 4)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data['Min Experience'], palette = 'magma')\n",
    "plt.xticks(fontsize = 9)\n",
    "plt.ylabel(\" \")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data['Max Experience'], palette = 'magma')\n",
    "plt.xticks(fontsize = 9)\n",
    "plt.ylabel(\" \")\n",
    "\n",
    "plt.suptitle('Distribution of Experience')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Education Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['education'] = data['education'].fillna('UG: Any Graduate - Any Specialization')\n",
    "data['education'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Education'] = data['education'].str.split(' ')\n",
    "data['Education'] = data['Education'].apply(lambda x: x[1] if len(x) > 1 else x[0])\n",
    "\n",
    "data['Education'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do some cleaning\n",
    "\n",
    "data['Education'] = data['Education'].replace(('B.Tech/B.E.','Graduation','Other','-','Not','B.Tech/B.E.,','Postgraduate',\n",
    "                                               'PG:CA','Diploma,','B.Com,','B.Pharma,','B.A,','BCA,','B.Sc,','MBA/PGDM','B.B.A,',\n",
    "                                              'PG:Other','Doctorate:Doctorate','Post'),\n",
    "                                              ('B.Tech','B.Tech','B.Tech','B.Tech','B.Tech','B.Tech','B.Tech',\n",
    "                                              'CA','Diploma','B.Com','B.Pharma','B.A','BCA','B.Sc','MBA','BBA',\n",
    "                                              'B.Tech','Doctorate','B.Tech'))\n",
    "\n",
    "data['Education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data['Education'].value_counts()\n",
    "data = data.loc[data['Education'].isin(counts.index[counts >= 25])]\n",
    "display(data['Education'].value_counts())\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12,6)\n",
    "x = data[data['Education'] != 'Any']\n",
    "sns.countplot(y = x['Education'], palette = 'inferno')\n",
    "plt.title('Vacancies for Different Education', fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.ylabel(\" \")\n",
    "plt.xlabel(\" \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Industry Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['industry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['industry'].str.split(' / ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['industry'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['industry'] = data['industry'].fillna(data['industry'].mode()[0])\n",
    "\n",
    "data['industry'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Industry'] = data['industry'].str.split(' / ')\n",
    "data['Industry'] = data['Industry'].apply(lambda x: x[0])\n",
    "\n",
    "display(data['Industry'].value_counts())\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 7)\n",
    "plt.title('Top Sectors for Jobs', fontsize = 20)\n",
    "sns.barplot(y = data['Industry'].value_counts().head(10).index,\n",
    "            x = data['Industry'].value_counts().head(10).values,\n",
    "            palette = 'copper')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.title('Minimum Experience required from each Industry')\n",
    "sns.barplot(data['Industry'], data['Min Experience'], palette = 'magma')\n",
    "plt.xticks(fontsize = 15, rotation = 90)\n",
    "plt.xlabel(' ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Skills Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['skills'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['skills'] = data['skills'].fillna(data['skills'].mode()[0])\n",
    "data['skills'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Skills'] = data['skills'].str.split(\" - \")\n",
    "data['Skills'] = data['Skills'].apply(lambda x: x[1] if len(x) > 1 else x[0])\n",
    "display(data['Skills'].value_counts())\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 4)\n",
    "plt.title('Requirement of Overall Skills', fontsize = 20)\n",
    "data['Skills'].value_counts().head(25).plot(kind = 'bar', color = 'black')\n",
    "plt.grid()\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.ylabel(\" \")\n",
    "plt.xlabel(\" \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Skills','numberofpositions']].groupby(['Skills']).agg('sum').sort_values(by = 'numberofpositions',\n",
    "                                 ascending = False).head(15).style.background_gradient(cmap = 'copper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning rest of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets remove the columns which are not required\n",
    "\n",
    "data = data.drop(['education',\n",
    "                  'joblocation_address',\n",
    "                  'experience',\n",
    "                  'industry',\n",
    "                  'skills',\n",
    "                  'jobid',\n",
    "                  'uniq_id',\n",
    "                  'site_name'], axis = 1)\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['numberofpositions'] = data['numberofpositions'].fillna(1)\n",
    "data['numberofpositions'] = data['numberofpositions'].astype('int')\n",
    "\n",
    "# If the job post date and pay rate is missing it is useless, so lets remove the records where jobppostdate is not mentioned\n",
    "data = data.dropna()\n",
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['postdate'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['postdate'] = data['postdate'].str.split(\" \")\n",
    "data['postdate'] = data['postdate'].apply(lambda x: x[0])\n",
    "data['postdate'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By looking at the above result we can easily analyze that there are some duplicate\n",
    "\n",
    "# lets print the no. of rows before removing Duplicates\n",
    "print(\"No. of Rows Before Removing Duplicates: \",data.shape[0])\n",
    "\n",
    "# so lets remove all the duplicates from the data\n",
    "data.drop_duplicates(subset = None, keep = 'first', inplace = True)\n",
    "\n",
    "# lets print the no. of rows after removing Duplicates\n",
    "print(\"No. of Rows After Removing Duplicates: \",data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Gather some Vital Information from Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['company'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['jobtitle'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Skills'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Industry'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['joblocation_adress'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['numberofpositions'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the most popular job titles\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(background_color = 'gold',\n",
    "                      max_words = 50,\n",
    "                      stopwords = stopwords,\n",
    "                      width = 2000,\n",
    "                      height = 2000).generate(str(data['jobtitle']))\n",
    "\n",
    "wordcloud2 = WordCloud(background_color = 'lightgrey',\n",
    "                       stopwords = stopwords,\n",
    "                       width = 2000,\n",
    "                       height = 2000).generate(str(data['company']))\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 7)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('Job Titles', fontsize = 20)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wordcloud2)\n",
    "plt.title('Companies', fontsize = 20)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.suptitle('Words Cloud', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the top 20 companies providing jobs\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 12)\n",
    "sns.barplot(y = data['company'].value_counts().head(30).index,\n",
    "            x = data['company'].value_counts().head(30).values,\n",
    "            palette = 'inferno')\n",
    "plt.title('Top Companies providing Jobs', fontsize = 20)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 150)\n",
    "@interact\n",
    "\n",
    "def check(City = list(data['joblocation_adress'].value_counts().index),\n",
    "          Industry = list(data['Industry'].value_counts().index),\n",
    "          Experience = list(data['Min Experience'].value_counts().sort_values(ascending = False).index),\n",
    "          Vacancies = 10\n",
    "          ):\n",
    "    return data.loc[(data['numberofpositions'] > Vacancies)\n",
    "                   & (data['joblocation_adress'] == City)\n",
    "                   & (data['Industry'] == Industry)\n",
    "                   & (data['Min Experience'] == Experience)][[\n",
    "                                                      'company',\n",
    "                                                      'jobtitle',\n",
    "                                                      'Education',\n",
    "                                                      'payrate',\n",
    "                                                      'numberofpositions',\n",
    "                                                      ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between Industries and Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a matrix of skills vs jobtitle\n",
    "# this matrix will be used for predicting the best suited job titles with respect to the skills\n",
    "x = pd.crosstab(data['Education'],data['Industry'])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def recommendation_jobs(Industry = list(data['Industry'].value_counts().index)):\n",
    "    job = x[Industry]\n",
    "    similar_jobs = x.corrwith(job)\n",
    "    similar_jobs = similar_jobs.sort_values(ascending=False)\n",
    "    similar_jobs = similar_jobs.iloc[2:]\n",
    "    return similar_jobs.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
