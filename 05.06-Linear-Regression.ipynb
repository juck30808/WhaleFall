{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [深入：朴素贝叶斯分类](05.05-Naive-Bayes.ipynb) | [目录](Index.ipynb) | [深入：支持向量机](05.07-Support-Vector-Machines.ipynb) >\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/wangyingsm/Python-Data-Science-Handbook/blob/master/notebooks/05.06-Linear-Regression.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Depth: Linear Regression\n",
    "# 深入：線性回歸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Just as naive Bayes is a good starting point for classification tasks, linear regression models are a good starting point for regression tasks.\n",
    "Such models are popular because they can be fit very quickly, and are very interpretable.\n",
    "You are probably familiar with the simplest form of a linear regression model (i.e., fitting a straight line to data) but such models can be extended to model more complicated data behavior.\n",
    "\n",
    "就像樸素貝葉斯是分類任務(classification) 的入門課，線性回歸模型是回歸任務(regression)的入門課。這種模型因為它能夠快速的訓練擬合以及非常容易解釋而流行。你可能已經了解了線性回歸模型的簡單形式（例如讓一條直線擬合到數據上），但是這樣的模型也能夠擴展到更加複雜的數據上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "## 簡單線性回歸\n",
    "\n",
    "> We will start with the most familiar linear regression, a straight-line fit to data.\n",
    "A straight-line fit is a model of the form where $a$ is commonly known as the *slope*, and $b$ is commonly known as the *intercept*.Consider the following data, which is scattered about a line with a slope of 2 and an intercept of -5:\n",
    "\n",
    "我們先從最熟悉的線性回歸模型開始，用一條直線擬合數據。一條擬合直線的模型具有下面的數學形式, 其中的$a$通常被稱為*斜率*，而$b$通常被成為*截距*。下面的數據是一些隨機散落在一條斜率(a)為2截距(b)為-5的直線附近的點：\n",
    "\n",
    "$$\n",
    "y = ax + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 5 + rng.randn(50)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can use Scikit-Learn's ``LinearRegression`` estimator to fit this data and construct the best-fit line:\n",
    "\n",
    "我們可以使用Scikit-Learn的`LinearRegression`評估器來擬合這些數據然後得到一條最佳擬合直線："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The slope and intercept of the data are contained in the model's fit parameters, which in Scikit-Learn are always marked by a trailing underscore.\n",
    "Here the relevant parameters are ``coef_`` and ``intercept_``:\n",
    "\n",
    "數據的斜率和截距可以在模型擬合參數中找到，在Scikit-Learn中總是使用下劃線後綴來表示。這里相關的參數是`coef_`和`intercept_`：正如我們所料，可以看到結果非常接近預期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model slope:    \", model.coef_[0])\n",
    "print(\"Model intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The ``LinearRegression`` estimator is much more capable than this, however—in addition to simple straight-line fits, it can also handle multidimensional linear models of the form where there are multiple $x$ values. Geometrically, this is akin to fitting a plane to points in three dimensions, or fitting a hyper-plane to points in higher dimensions.\n",
    "\n",
    "`LinearRegression`評估器能做的遠不止於此，除了簡單的直線擬合外，它還能處理多維線性模型的形式，這裡有多個$x$值。幾何上，這等同於在三維空間間使用一個平面擬合數據，或在更高維空間中使用超平面擬合數據。\n",
    "\n",
    "$$\n",
    "y = a_0 + a_1 x_1 + a_2 x_2 + \\cdots\n",
    "$$\n",
    "\n",
    "\n",
    "> The multidimensional nature of such regressions makes them more difficult to visualize, but we can see one of these fits in action by building some example data, using NumPy's matrix multiplication operator:\n",
    "\n",
    "這樣的回歸具有多維的本質，因此令它們比較難以可視化，但我們可以構造一些樣例數據來查看這樣的擬合，這裡使用了NumPy的矩陣乘法操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = 10 * rng.rand(100, 3)\n",
    "y = 0.5 + np.dot(X, [1.5, -2., 1.])\n",
    "\n",
    "model.fit(X, y)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here the $y$ data is constructed from three random $x$ values, and the linear regression recovers the coefficients used to construct the data.In this way, we can use the single ``LinearRegression`` estimator to fit lines, planes, or hyperplanes to our data.\n",
    "It still appears that this approach would be limited to strictly linear relationships between variables, but it turns out we can relax this as well.\n",
    " \n",
    "這裡 $y$ 值是由三個隨機 $x$ 值構建的，而線性回歸恢復了用來構建數據的斜率。使用這種方法，我們可以使用單個`LinearRegression`評估器擬合直線、平面或超平面到數據上。目前為止這種方法看起來都限制在變量之間的線性關聯上，但是實際上它還能完成更多的工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_excel('input/regression_data.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(\"x\", \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 擬合數據得到一條最佳直線\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = df.iloc[:,:-1].values # converting to array\n",
    "y = df.iloc[:,-1].values \n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X,y)\n",
    "\n",
    "# intercept , slope\n",
    "print(\"Model slope:    \", model.coef_[0])\n",
    "print(\"Model intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "y_pred_sk = model.predict(X)\n",
    "x_test = np.array([3]).reshape(-1,1)\n",
    "model.predict(x_test)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(df['x'],df['y'],'bo',markersize=10,markerfacecolor='w')\n",
    "# linear regression line\n",
    "plt.plot(df['x'],y_pred_sk,'go',markersize=15,markerfacecolor='w',alpha=0.8)\n",
    "plt.plot(df['x'],y_pred_sk,'g')\n",
    "plt.plot(df['x'],y_pred_sk,'go')\n",
    "\n",
    "plt.xlabel('independent')\n",
    "plt.ylabel('dependent')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data=pd.read_csv(\"input/Salary_Data.csv\")\n",
    "data.head()\n",
    "# plt.scatter(data[\"YearsExperience\"],data[\"Salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearRegression 擬合數據得到一條最佳直線\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = data.iloc[:,:-1].values # converting to array\n",
    "Y = data.iloc[:,-1].values \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "model=LinearRegression()\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 進行預測標籤\n",
    "X_test=X_test.reshape(-1,1)\n",
    "Y_predicted=model.predict(X_test)\n",
    "Y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model slope:    \", model.coef_[0])\n",
    "print(\"Model intercept:\", model.intercept_)\n",
    "\n",
    "plt.scatter(X_train,Y_train,color='blue')\n",
    "plt.plot(X_train,model.predict(X_train),color='red')\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.xlabel('No of Years')\n",
    "plt.ylabel('Salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Process:\n",
    "1. Import the model you want to use\n",
    "2. Create an instance of that model and set any hyperparameters you want\n",
    "3. Fit the model to the data, this computes the parameters of the model using machine learning\n",
    "4. Predict new information using the model\n",
    "\n",
    "默認情況下，LinearRegression 模型將擬合 y 截距，但由於我們不想做出該假設，因此我們明確地傳遞 fit_intercept=True。 設定為預先設置而不是讓它們從數據中學習。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "data = read_csv(\"https://milliams.com/courses/applied_data_analysis/linear.csv\")\n",
    "data.head()\n",
    "#data.plot.scatter(\"x\", \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(data[[\"x\"]], data[\"y\"])\n",
    "\n",
    "print(\"Model gradient: \", model.coef_[0])   #陡度\n",
    "print(\"Model intercept:\", model.intercept_) #截距\n",
    "\n",
    "# 取得方程式 y = 1.97x − 4.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "x_fit = pd.DataFrame({\"x\": [data[\"x\"].min(), data[\"x\"].max()]})\n",
    "y_pred = model.predict(x_fit)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data.plot.scatter(\"x\", \"y\", ax=ax)\n",
    "ax.plot(x_fit[\"x\"], y_pred, linestyle=\":\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - 體位身高與體重的分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  #csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dt = pd.read_csv(\"input/500_Person_Gender_Height_Weight_Index.csv\")\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi_health_3 = dt[dt[\"Index\"] == 3]   # made Frame\n",
    "#bmi_health_3 = dt[\"Index\"] == 3     # None Frame\n",
    "bmi_health_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bmi_health_3['Height'], bmi_health_3['Weight'], \"b.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis Function Regression\n",
    "\n",
    "## 基本函數回歸\n",
    "\n",
    "> One trick you can use to adapt linear regression to nonlinear relationships between variables is to transform the data according to *basis functions*.\n",
    "We have seen one version of this before, in the ``PolynomialRegression`` pipeline used in [Hyperparameters and Model Validation](05.03-Hyperparameters-and-Model-Validation.ipynb) and [Feature Engineering](05.04-Feature-Engineering.ipynb).\n",
    "The idea is to take our multidimensional linear model: and build the $x_1, x_2, x_3,$ and so on, from our single-dimensional input $x$. That is, we let $x_n = f_n(x)$, where $f_n()$ is some function that transforms our data.\n",
    "\n",
    "將線性回歸應用在變量之間的非線性關係的一個技巧是，將數據通過*基本函數*進行轉換。我們在[超參數和模型驗證](05.03-Hyperparameters-and-Model-Validation.ipynb)和[特徵工程](05.04-Feature-Engineering.ipynb)中已經看到過多項式回歸`PolynomialRegression`管道操作中已經看到這個技巧的例子。這個方法是將一維的輸入數據使用多維線性模型來建立$x_1, x_2, x_3$等。即我們令$x_n = f_n(x)$其中的$f_n()$是用來轉換數據的函數。\n",
    "\n",
    "> For example, if $f_n(x) = x^n$, our model becomes a polynomial regression:Notice that this is *still a linear model*—the linearity refers to the fact that the coefficients $a_n$ never multiply or divide each other.\n",
    "What we have effectively done is taken our one-dimensional $x$ values and projected them into a higher dimension, so that a linear fit can fit more complicated relationships between $x$ and $y$.\n",
    "\n",
    "例如，如果令$f_n(x) = x^n$，我們的模型就會變成一個多項式回歸：注意這裡模型仍然是*線性的*，線性的意思是指模型中的斜率$a_n$沒有互相進行乘法或除法操作。這裡起作用的是我們將一維的$x$值投射到了更高的維度上，這樣我們的線性模型就能擬合$x$和$y$之間更加複雜的聯繫。\n",
    "\n",
    "$$\n",
    "y = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \\cdots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多元回歸(multiple regression)\n",
    "當資料的欄位不只一個時建立的線性回歸就稱為多元回歸\n",
    "\n",
    "$$\n",
    "y = w_1* x 1 + w_2* x 2 + \\cdots + w_n*x_n + b\n",
    "$$\n",
    "* y : 預測值\n",
    "* x1 ~ xn : 每個欄位的值\n",
    "* w1 ~ wn : 每個欄位的斜率\n",
    "* b : 基底\n",
    "\n",
    "w1~wn是每個欄位(項度)的斜率，也可以看成是每個欄位對於預測輸出的重要性\n",
    "* 遇到非線性資料預測會失準\n",
    "* 會受到離異值的影響"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial basis functions 多項式基本函數\n",
    "\n",
    "> This polynomial projection is useful enough that it is built into Scikit-Learn, using the ``PolynomialFeatures`` transformer:\n",
    "\n",
    "這種多項式投射如此有用，所以Scikit-Learn內建了實現它的方法，就是`PolynomialFeatures`轉換："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "x = np.array([2, 3, 4])\n",
    "poly = PolynomialFeatures(3, include_bias=False)\n",
    "poly.fit_transform(x[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We see here that the transformer has converted our one-dimensional array into a three-dimensional array by taking the exponent of each value. This new, higher-dimensional data representation can then be plugged into a linear regression. As we saw in [Feature Engineering](05.04-Feature-Engineering.ipynb), the cleanest way to accomplish this is to use a pipeline.\n",
    "Let's make a 7th-degree polynomial model in this way:\n",
    "\n",
    "我們看到上例中使用這個轉換器我們對每個值求冪將一維數組變成了三維數組。這個新的高維數據表示能應用到線性回歸中。正如我們在[特徵工程](05.04-Feature-Engineering.ipynb)中看到的，實現這個任務的最優雅犯法是使用管道。這裡我們創建一個7階的多項式模型，使得我們的線性模型能夠異常良好的擬合到這個非線性數據上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "poly_model = make_pipeline(PolynomialFeatures(7), LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With this transform in place, we can use the linear model to fit much more complicated relationships between $x$ and $y$. \n",
    "For example, here is a sine wave with noise:\n",
    "\n",
    "有了這樣的轉換方式，我們可以使用線性模型來擬合複雜得多的$x$和$y$的關係。例如像下面的帶有噪音的正弦波："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = np.sin(x) + 0.1 * rng.randn(50)\n",
    "\n",
    "poly_model.fit(x[:, np.newaxis], y)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian basis functions 高斯基本函數\n",
    "\n",
    "> Of course, other basis functions are possible.\n",
    "For example, one useful pattern is to fit a model that is not a sum of polynomial bases, but a sum of Gaussian bases.\n",
    "The result might look something like the following figure:\n",
    "\n",
    "當然還有其他可用的基本函數。例如可以通過高斯函數疊加而不是多項式疊加來擬合模型。結果可能如下圖所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.06-gaussian-basis.png)\n",
    "[附录中生成图像的代码](#Gaussian-Basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The shaded regions in the plot are the scaled basis functions, and when added together they reproduce the smooth curve through the data.\n",
    "These Gaussian basis functions are not built into Scikit-Learn, but we can write a custom transformer that will create them, as shown here and illustrated in the following figure (Scikit-Learn transformers are implemented as Python classes; reading Scikit-Learn's source is a good way to see how they can be created):\n",
    "\n",
    "上圖中陰影部分是基本函數的覆蓋範圍，當這些陰影疊加在一起時就會產生上面光滑的擬合曲線。 Scikit-Learn中沒有內建這些高斯基本函數，但我們可以寫一個自定義的轉換器來構造它們，Scikit-Learn的轉換器是使用Python類實現的；閱讀Scikit-Learn的源代碼是理解它們創建的好方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GaussianFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"對一維數據進行均勻分佈高斯轉換\"\"\"\n",
    "    \n",
    "    def __init__(self, N, width_factor=2.0):\n",
    "        self.N = N\n",
    "        self.width_factor = width_factor\n",
    "    \n",
    "    @staticmethod\n",
    "    def _gauss_basis(x, y, width, axis=None):\n",
    "        arg = (x - y) / width\n",
    "        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # 沿著數據范圍創建均勻分佈的N個中心點\n",
    "        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n",
    "        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return self._gauss_basis(X[:, :, np.newaxis], self.centers_,self.width_, axis=1)\n",
    "    \n",
    "gauss_model = make_pipeline(GaussianFeatures(20),LinearRegression())\n",
    "gauss_model.fit(x[:, np.newaxis], y)\n",
    "yfit = gauss_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit)\n",
    "plt.xlim(0, 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "## 正則化\n",
    "\n",
    "> The introduction of basis functions into our linear regression makes the model much more flexible, but it also can very quickly lead to over-fitting (refer back to [Hyperparameters and Model Validation](05.03-Hyperparameters-and-Model-Validation.ipynb) for a discussion of this).For example, if we choose too many Gaussian basis functions, we end up with results that don't look so good:\n",
    "\n",
    "將基本函數引入線性回歸令模型更加靈活，但是它很容易導致過擬合（參見[超參數和模型驗證](05.03-Hyperparameters-and-Model-Validation.ipynb)）。如果選擇太多高斯函數，產生的結果就不太可靠："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(GaussianFeatures(30), LinearRegression())\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.5, 1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the data projected to the 30-dimensional basis, the model has far too much flexibility and goes to extreme values between locations where it is constrained by data.We can see the reason for this if we plot the coefficients of the Gaussian bases with respect to their locations:The lower panel of this figure shows the amplitude of the basis function at each location.This is typical over-fitting behavior when basis functions overlap: the coefficients of adjacent basis functions blow up and cancel each other out.We know that such behavior is problematic, and it would be nice if we could limit such spikes expliticly in the model by penalizing large values of the model parameters.Such a penalty is known as *regularization*, and comes in several forms.\n",
    "\n",
    "投射到30維的空間上，該模型太過於靈活以至於當處於間隔距離較大的點之間的位置時候會擬合成很極端的數據值。我們可以將高斯函數的係數也繪製在圖表中，就可以看到原因：下面的圖展示了基本函數在每個位置的振幅。這是當使用基本函數疊加的典型過擬合情況：鄰近的基本函數的係數互相疊加到波峰和波谷。這種情形是錯誤的，如果我們能在模型中限制這樣的尖刺能解決這個問題，這樣被成為*正則化*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def basis_plot(model, title=None):\n",
    "    fig, ax = plt.subplots(2, sharex=True)\n",
    "    model.fit(x[:, np.newaxis], y)\n",
    "    ax[0].scatter(x, y)\n",
    "    ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
    "    ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))\n",
    "    \n",
    "    if title:\n",
    "        ax[0].set_title(title)\n",
    "\n",
    "    ax[1].plot(model.steps[0][1].centers_,\n",
    "               model.steps[1][1].coef_)\n",
    "    ax[1].set(xlabel='basis location',\n",
    "              ylabel='coefficient',\n",
    "              xlim=(0, 10))\n",
    "    \n",
    "model = make_pipeline(GaussianFeatures(30), LinearRegression())\n",
    "basis_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization - penalty parameter, P\n",
    "\n",
    "###  正規化 - 懲罰參數\n",
    "常見的懲罰係數有兩種(分別對應到 Ridge & Lasso)，效果類似。懲罰係數將會限制回歸係數的大小，除非該變數可以使誤差平方和(SSE)降低對應水準，該特徵係數才會上升。以下就來進一步介紹兩種最常見的正規化回歸法。正規化回歸的目標函數與 OLS 回歸類似，但多了一個懲罰參數 (Penalty parameter)\n",
    "\n",
    "- Ridge：如果我們的數據集少或特徵和特徵之間的相關性很小，那麼我們應該選擇 Ridge 模型，他只會將係數逼近到接近零（但不會真的是0）\n",
    "- 若數據少情況下使用 Lassalle，那麼少數特徵的權重將變為 0，即會導致數據丟失，最終削弱我們的模型。\n",
    "\n",
    "$$\n",
    "minnimize (SSE + P)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression ($L_2$ Regularization) 嶺回歸（$L_2$正則化）\n",
    "\n",
    "> The most common form of regularization is known as *ridge regression* or $L_2$ *regularization*, sometimes also called *Tikhonov regularization*. This proceeds by penalizing the sum of squares (2-norms) of the model coefficients; in this case, the penalty on the model fit would be where $\\alpha$ is a free parameter that controls the strength of the penalty. This type of penalized model is built into Scikit-Learn with the ``Ridge`` estimator:\n",
    "\n",
    "最常用的正則化方式：*嶺回歸* ($L_2$*)* 也被叫*Tikhonov正則化*。這個過程通過對模型係數的平方和（2-範數）進行乘法，也就是會對係數做二階懲罰(L2) ；在這個例子中，其中$\\alpha$是控制乘法力度的參數。這類的懲罰模型內建在Scikit-Learn中`Ridge`評估器中：\n",
    "$$\n",
    "P = \\alpha\\sum_{n=1}^N \\theta_n^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The $\\alpha$ parameter is essentially a knob controlling the complexity of the resulting model.\n",
    "In the limit $\\alpha \\to 0$, we recover the standard linear regression result; in the limit $\\alpha \\to \\infty$, all model responses will be suppressed.\n",
    "One advantage of ridge regression in particular is that it can be computed very efficiently—at hardly more computational cost than the original linear regression model.\n",
    "\n",
    "$\\alpha$參數是用來控制模型複雜度的關鍵開關。極限情況$\\alpha \\to 0$時，恢復到標準線性回歸結果；極限情況$\\alpha \\to \\infty$時，所有模型的響應都會被壓縮。嶺回歸的一大優點是它能非常有效的計算，基本沒有產生比原始線性回歸模型更大的計算消耗。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "model = make_pipeline(GaussianFeatures(30), Ridge(alpha=0.1))\n",
    "basis_plot(model, title='Ridge Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression ($L_1$ regularization)  Lasso算法回歸（$L_1$正則化）\n",
    "\n",
    "> Another very common type of regularization is known as lasso, and involves penalizing the sum of absolute values (1-norms) of regression coefficients:Though this is conceptually very similar to ridge regression, the results can differ surprisingly: for example, due to geometric reasons lasso regression tends to favor *sparse models* where possible: that is, it preferentially sets model coefficients to exactly zero.  We can see this behavior in duplicating the ridge regression figure, but using L1-normalized coefficients:\n",
    "\n",
    "另一個常用的正則化類型被稱為lasso，通過懲罰回歸係數絕對值和（1-範數）來實現：雖然這在概念上非常類似嶺回歸，但是結果卻大不相同：例如因為幾何原因lasso回歸更適合*稀疏模型*，即它傾向於將模型係數設置為0。我們可以從下面的圖中看到這個特點，這裡將嶺回歸改成了使用L1正則化係數：\n",
    "\n",
    "$$\n",
    "P = \\alpha\\sum_{n=1}^N |\\theta_n|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the lasso regression penalty, the majority of the coefficients are exactly zero, with the functional behavior being modeled by a small subset of the available basis functions.\n",
    "As with ridge regularization, the $\\alpha$ parameter tunes the strength of the penalty, and should be determined via, for example, cross-validation (refer back to [Hyperparameters and Model Validation](05.03-Hyperparameters-and-Model-Validation.ipynb) for a discussion of this).\n",
    "\n",
    "使用了lasso回歸懲罰，大部分的係數都變成了0，也就是只有小部分的基本函數在模型中產生了作用。就像嶺回歸正則化，$\\alpha$參數調整懲罰的強度，這個參數應該通過諸如交叉驗證（參見[超參數和模型驗證](05.03-Hyperparameters-and-Model-Validation.ipynb)）來確定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model = make_pipeline(GaussianFeatures(30), Lasso(alpha=0.001, tol=0.01))\n",
    "basis_plot(model, title='Lasso Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net模型\n",
    "雖然Lasso模型會執行變數挑選，但一個源自於懲罰參數的結果就是，通常當兩個高度相關的變數的係數在被逼近成為0的過程中，可能一個會完全變成0但另為一個仍保留在模型中。此外，這種一個在內、一個在外的處理方法不是很有系統。相對的，Ridge模型的懲罰參數就稍具效率一點，可以有系統的將高相關性變數的係數一起降低。於是Elastic Net模型的優勢就在於，它綜合了 Ridge Penalty 達到有效正規化優勢以及 Lasso Penalty 能夠進行變數挑選優勢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P = (L2) + (L1) = \\alpha\\sum_{n=1}^N \\theta_n^2 + \\alpha\\sum_{n=1}^N |\\theta_n| \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_excel(\"input/insurance.xlsx\")\n",
    "data.head()\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_types = dict(data.dtypes)\n",
    "for name , type_ in d_types.items():\n",
    "    if str(type_) == 'object':\n",
    "        print(f\"======== {name} ===========\")\n",
    "        print(data[name].value_counts())\n",
    "        #We see that we have 3 objective data in our data frame. (sex, smoker, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "for name , type_ in d_types.items():\n",
    "    if str(type_) == 'object':\n",
    "        Le = LabelEncoder()\n",
    "        data[name] = Le.fit_transform(data[name])\n",
    "        \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder()\n",
    "part = onehotencoder.fit_transform(data['region'].values.reshape(-1,1)).toarray()\n",
    "values = dict(data[\"region\"].value_counts())\n",
    "for e , (val , _) in enumerate(values.items()):\n",
    "    data[\"region_\" + str(val)] = part[:,e]\n",
    "data = data.drop([\"region\"] , axis = 1)\n",
    "\n",
    "\n",
    "from scipy import stats \n",
    "col_cox , lam = stats.boxcox(data[\"expenses\"])[0:2]\n",
    "data[\"expenses\"] = col_cox\n",
    "remaining_columns = list(data.columns)\n",
    "remaining_columns.remove(\"expenses\")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data[remaining_columns].values \n",
    "Y = data['expenses'].values\n",
    "Xtrain , Xtest , Ytrain , Ytest = train_test_split(X , Y , test_size = 0.2 , random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Regularized Regression 正規化回歸\n",
    "對回歸係數大小做出約束，並逐漸的將回歸係數壓縮到零。而對回歸係數的限制將有助於降低係數的幅度和波動，並降低模型的變異。\n",
    "'''\n",
    "from sklearn.linear_model import Ridge , Lasso , ElasticNet , LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(Xtrain , Ytrain)\n",
    "print(\" Training Accuracy : \" , model.score(Xtrain , Ytrain))\n",
    "print(\" Testing Accuracy : \" , model.score(Xtest , Ytest))    #0.74970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge 模型只會將係數逼近到接近零（但不會真的是0） Scoure 0.7502 better than Regression\n",
    "model = Ridge()\n",
    "model.fit(Xtrain , Ytrain)\n",
    "print(\" Training Accuracy : \" , model.score(Xtrain , Ytrain))\n",
    "print(\" Testing Accuracy : \" , model.score(Xtest , Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso模型真的會將係數推進成0\n",
    "model = Lasso()\n",
    "model.fit(Xtrain , Ytrain)\n",
    "print(\" Training Accuracy : \" , model.score(Xtrain , Ytrain))\n",
    "print(\" Testing Accuracy : \" , model.score(Xtest , Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic \n",
    "model = ElasticNet()\n",
    "model.fit(Xtrain , Ytrain)\n",
    "print(\" Training Accuracy : \" , model.score(Xtrain , Ytrain))\n",
    "print(\" Testing Accuracy : \" , model.score(Xtest , Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Predicting Bicycle Traffic 預測自行車流量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> As an example, let's take a look at whether we can predict the number of bicycle trips across Seattle's Fremont Bridge based on weather, season, and other factors.\n",
    "We have seen this data already in [Working With Time Series](03.11-Working-with-Time-Series.ipynb).\n",
    "\n",
    "我們來看一個例子，試圖從天氣、季節和其他因素中對西雅圖費利蒙大橋的自行車交通流量數據進行預測。我們已經在[在時間序列上操作](03.11-Working-with-Time-Series.ipynb)一節中使用過這個數據。\n",
    "\n",
    "> In this section, we will join the bike data with another dataset, and try to determine the extent to which weather and seasonal factors—temperature, precipitation, and daylight hours—affect the volume of bicycle traffic through this corridor.\n",
    "Fortunately, the NOAA makes available their daily [weather station data](http://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND) (I used station ID USW00024233) and we can easily use Pandas to join the two data sources.\n",
    "We will perform a simple linear regression to relate weather and other information to bicycle counts, in order to estimate how a change in any one of these parameters affects the number of riders on a given day.\n",
    "\n",
    "本節中，我們會將自行車數據與另外一個數據集聯合起來，然後從中找到哪些天氣和季節因素，比方說溫度、降雨和日照時間，會影響到這條交通要道自行車流量數據。幸運的是美國國家海洋和大氣管理局NOAA公開了每天[氣象站數據](http://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND)（ID USW00024233），我們可以使用Pandas很容易地聯合兩個數據集。然後我們使用簡單的線性回歸來擬合相關的天氣以及其他因素和自行車數量，以此來估計給定一天的任何其中一個參數改變對騎行者數量的影響。\n",
    "\n",
    "> In particular, this is an example of how the tools of Scikit-Learn can be used in a statistical modeling framework, in which the parameters of the model are assumed to have interpretable meaning.\n",
    "As discussed previously, this is not a standard approach within machine learning, but such interpretation is possible for some models.Let's start by loading the two datasets, indexing by date:\n",
    "\n",
    "特別這是在統計模型框架中使用Scikit-Learn工具的例子，其中的模型參數被認為是有可解釋的含義的。正如之前討論的，這不是機器學期的標準方法，但是對於一些模型來說這樣的解釋是存在的。讓我們首先載入兩個數據集，使用日期進行索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -o FremontBridge.csv https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "counts = pd.read_csv('input/FremontBridge.csv', index_col='Date', parse_dates=True)\n",
    "weather = pd.read_csv('input/BicycleWeather.csv', index_col='DATE', parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> we will compute the total daily bicycle traffic, and put this in its own dataframe:\n",
    "\n",
    "我們計算每天自行車的總流量，把這個數據放進它自己的DataFrame中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = counts.resample('d').sum()\n",
    "daily['Total'] = daily.sum(axis=1)\n",
    "daily = daily[['Total']] # 移除其他列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We saw previously that the patterns of use generally vary from day to day; let's account for this in our data by adding binary columns that indicate the day of the week:\n",
    "\n",
    "我們之前看到自行車流量隨著星期天數而發生不同變化；因此讓我們將這點也考慮進來，為這個數據集增加7個布爾值的列表示星期天數："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "for i in range(7):\n",
    "    daily[days[i]] = (daily.index.dayofweek == i).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Similarly, we might expect riders to behave differently on holidays; let's add an indicator of this as well:\n",
    "\n",
    "類似的，我們也期望騎手們在節日會有不同習慣；讓我們將這點也考慮進來，加入一個標識列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays('2012', '2016')\n",
    "daily = daily.join(pd.Series(1, index=holidays, name='holiday'))\n",
    "daily['holiday'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We also might suspect that the hours of daylight would affect how many people ride;\n",
    "\n",
    "我們同樣猜測日照時間也會影響多少人騎自行車："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def hours_of_daylight(date, axis=23.44, latitude=47.61):\n",
    "    \"\"\"\n",
    "    計算給定日期的日照時間\n",
    "    axis 23.44 黃赤夾角\n",
    "    latitude 47.61 西雅圖緯度\n",
    "    \"\"\"\n",
    "    # 2000年12月21日是冬至日，日照時間最短\n",
    "    days = (date - datetime(2000, 12, 21)).days\n",
    "    m = (1. - np.tan(np.radians(latitude))\n",
    "         * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\n",
    "    return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.\n",
    "\n",
    "daily['daylight_hrs'] = list(map(hours_of_daylight, daily.index))\n",
    "daily[['daylight_hrs']].plot()\n",
    "plt.ylim(8, 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can also add the average temperature and total precipitation to the data.\n",
    "In addition to the inches of precipitation, let's add a flag that indicates whether a day is dry (has zero precipitation):\n",
    "\n",
    "我們也可以增加平均氣溫和總降雨量數據。除了單位為英寸的降雨量列外，我們再增加一列標誌表示當天是否乾燥（降雨量為0）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 氣溫單位是0.1攝氏度，求平均值\n",
    "weather['TMIN'] /= 10\n",
    "weather['TMAX'] /= 10\n",
    "weather['Temp (C)'] = 0.5 * (weather['TMIN'] + weather['TMAX'])\n",
    "\n",
    "# 降雨量單位是0.1毫米，轉換為英寸\n",
    "weather['PRCP'] /= 254\n",
    "weather['dry day'] = (weather['PRCP'] == 0).astype(int)\n",
    "\n",
    "daily = daily.join(weather[['PRCP', 'Temp (C)', 'dry day']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finally, let's add a counter that increases from day 1, and measures how many years have passed.\n",
    "This will let us measure any observed annual increase or decrease in daily crossings:\n",
    "\n",
    "最後，讓我們增加一列計數器從第一天開始計數，然後轉換成經過的年的小數數值。該列會在每年進行循環："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily['annual'] = (daily.index - daily.index[0]).days / 365.\n",
    "daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With this in place, we can choose the columns to use, and fit a linear regression model to our data.\n",
    "We will set ``fit_intercept = False``, because the daily flags essentially operate as their own day-specific intercepts: Finally, we can compare the total and predicted bicycle traffic visually:\n",
    "\n",
    "有了數據後，我們可以選擇使用哪些列來讓線性回歸模型進行擬合。我們設置`fit_intercept=False`，因為每天的數據都有著那一天自己的截距：最終我們將預測的自行車交通流量和實際總量進行比較繪製圖表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除所有有空值的行\n",
    "daily.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# 用來擬合模型的列包括星期幾、日照小時數、降水量、是否有雨、氣溫、該天的年計數\n",
    "column_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'holiday',\n",
    "                'daylight_hrs', 'PRCP', 'dry day', 'Temp (C)', 'annual']\n",
    "X = daily[column_names]\n",
    "y = daily['Total']\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    "daily['predicted'] = model.predict(X)\n",
    "daily[['Total', 'predicted']].plot(alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is evident that we have missed some key features, especially during the summer time.\n",
    "Either our features are not complete (i.e., people decide whether to ride to work based on more than just these) or there are some nonlinear relationships that we have failed to take into account (e.g., perhaps people ride less at both high and low temperatures).\n",
    "Nevertheless, our rough approximation is enough to give us some insights, and we can take a look at the coefficients of the linear model to estimate how much each feature contributes to the daily bicycle count:\n",
    "\n",
    "很明顯我們遺失了一些關鍵的特徵，特別是在夏天的時候。或者我們的特徵不完整（如決定人們是否騎行的因素不止上述那些特徵）或者數據之間具有非線性的關係我們並未考慮進來（如人們在高溫和低溫的情況下都會減少騎行）。無論如何，我們這個粗糙的估計給了我們一些內在解釋，我們可以查看這個線性模型的係數，從中得到每個特徵是如何影響每天自行車總量的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.Series(model.coef_, index=X.columns)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These numbers are difficult to interpret without some measure of their uncertainty.\n",
    "We can compute these uncertainties quickly using bootstrap resamplings of the data:\n",
    "\n",
    "這些數字如果沒有一種對它們不確定性的度量方式的話很難解讀。我們可以使用對數據的重採樣來快速的計算這些不確定性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "np.random.seed(1)\n",
    "err = np.std([model.fit(*resample(X, y)).coef_\n",
    "              for i in range(1000)], 0)\n",
    "\n",
    "print(pd.DataFrame({'effect': params.round(0),\n",
    "                    'error': err.round(0)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We first see that there is a relatively stable trend in the weekly baseline: there are many more riders on weekdays than on weekends and holidays.\n",
    "We see that for each additional hour of daylight, 129 ± 9 more people choose to ride; a temperature increase of one degree Celsius encourages 65 ± 4 people to grab their bicycle; a dry day means an average of 548 ± 33 more riders, and each inch of precipitation means 665 ± 62 more people leave their bike at home.\n",
    "Once all these effects are accounted for, we see a modest increase of 27 ± 18 new daily riders each year.\n",
    "\n",
    "首先看到的是每週相對穩定的變化趨勢：顯然工作日比周末的騎行者要多得多。如果每天日照時間多一個小時，就會多出240.0 ± 31.0個騎行者；氣溫升高一攝氏度會多出135.0 ± 10.0個騎行者；晴天意味著會多出1032.0 ± 103.0個騎行者；而每多一英寸降雨意味著會有1389.0 ± 175.0個人決定將自行車留在家。一旦所有因素都計算在內，我們發現每年同一天會平均多出38.0 ± 109.0個騎行者。\n",
    "> Our model is almost certainly missing some relevant information. For example, nonlinear effects (such as effects of precipitation *and* cold temperature) and nonlinear trends within each variable (such as disinclination to ride at very cold and very hot temperatures) cannot be accounted for in this model.\n",
    "Additionally, we have thrown away some of the finer-grained information (such as the difference between a rainy morning and a rainy afternoon), and we have ignored correlations between days (such as the possible effect of a rainy Tuesday on Wednesday's numbers, or the effect of an unexpected sunny day after a streak of rainy days).\n",
    "These are all potentially interesting effects, and you now have the tools to begin exploring them if you wish!\n",
    "\n",
    "我們的模型基本可以肯定遺漏了一些相關的信息。例如，非線性效果（比方說降水量*和*低氣溫的共同作用）和每個變量的非線性趨勢（比方說在非常熱和非常冷的天氣下騎車的縮減量），這個模型都沒有計算在內。除此之外，我們還拋棄了一些細顆粒度的信息（例如下雨早晨和下雨下午的區別），而且我們還忽略了連續天數之間的關聯（比方說預報週三下雨結果週二就下雨了或者是連續雨天后的一個意料外的晴天）。這些都是潛在有趣的效應，並且你現在已經有了能夠進一步探索它們的工具了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [深入：朴素贝叶斯分类](05.05-Naive-Bayes.ipynb) | [目录](Index.ipynb) | [深入：支持向量机](05.07-Support-Vector-Machines.ipynb) >\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/wangyingsm/Python-Data-Science-Handbook/blob/master/notebooks/05.06-Linear-Regression.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "np.set_printoptions(precision=2) #.2\n",
    "rng = np.random.RandomState(1)\n",
    " \n",
    "# 初始化實驗數據並減去平均\n",
    "W = rng.rand(2, 2)\n",
    "X_normal = rng.normal(scale=5, size=(2, 20))\n",
    "X_orig = W @ X_normal  # @ 就是你學過的矩陣相乘運算\n",
    "X_mean = X_orig.mean(axis=1)[:, np.newaxis]\n",
    "X = X_orig - X_mean\n",
    "mean = X.mean(axis=1)\n",
    "\n",
    "# 測試 numerical 相等，確保樣本的平均已經為 0 實作演算法時十分重要\n",
    "assert_almost_equal(0, mean)\n",
    "print('X.shape:', X.shape, '\\n')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "每個樣本為一個 column vector，索引從 0 開始\n",
    "第一個 「 : 」 代表取得所有對應的 rows\n",
    "\"\"\"\n",
    "X[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "除了 NumPy 比較特別以外，有實際用過 scikit-learn、PyTorch 或是 TensorFlow 做過矩陣運算的讀者們應該都清楚，\n",
    "實作上這些函式庫常會將數據矩陣 X 做轉置（transpose），使其維度變成 (n_samples, n_features)。這樣的好處是\n",
    "每一個列向量（row vector）都直接對應到一個樣本。這使得我們可以更輕鬆地存取特定樣本：\n",
    "\"\"\"\n",
    "# sanity check\n",
    "assert_almost_equal(X[:, 0], X.T[0])\n",
    "X.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "array([[ 2.89,  0.32,  5.8 , -6.52,  3.94, -4.21],    <- n_features *2\n",
    "       [ 1.52,  0.91,  1.52, -0.88, -0.03, -1.26]])\n",
    "                        ^\n",
    "                    n_samples*6\n",
    "\"\"\"\n",
    "X[:, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 笛卡爾座標系統(Cartesian coordinate system)\n",
    "\n",
    "現在想像你興沖沖地跑去見指導教授，迫不及待地獻上你剛搜集到的熱騰騰數據 X。教授僅看了一眼便道：\n",
    "\n",
    "- 兩個特徵有點多，你能不能想辦法只用一個特徵來表示這些樣本的特性？\n",
    "\n",
    "你連忙點頭稱是，接著便離開教授的辦公室。回到螢幕前，你盯著 X 裡頭的這些數字 #越想越不對勁。到底要怎樣才能把這些 2 維向量 x 各自用一個新的數值表示，同時又能保持這些樣本的特性不變呢？僅僅是將看似毫無章法的數據 X 描繪在這個座標系統上面，我們就能透過與生俱來的幾何直覺預測兩特徵 f1 與 f2 之間存在著某種程度的線性關係。這是幾何觀點上的一大勝利。這個發現讓我們離降維的目標近了許多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[0], X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# 第一個參數為所有的 xs, 第二個參數為所有的 ys\n",
    "plt.scatter(X[0, :], X[1, :])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量投影到某低維子空間，事實上就是在線性地降低其維度。事實上這就是線性降維與 PCA 的核心精神：將原始數據拆解成更具代表性的主成分，並以其作為新的基準，重新描述數據。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 該直線的單位向量（顯示到小數後第兩位）\n",
    "v = np.array([0.9691344, 0.246533])\n",
    "print(\"v       :\", v)  # shape: (2,)\n",
    "assert_almost_equal(1, np.linalg.norm(v))\n",
    "\n",
    "# 使用 v 建立投影矩陣 Ｐ1\n",
    "# 因為 P 是將 X 投影到 1 維，因此加個 1 在後面\n",
    "P1 = v[np.newaxis, :]  # shape: (1, 2)\n",
    "print(\"P1      :\", P1)\n",
    "\n",
    "# 利用 P1 將數據 X 投影到 v 所在子空間 \n",
    "L = P1 @ X\n",
    "\n",
    "# 前 4 個樣本的新特徵 L 跟動畫內結果相同\n",
    "print(\"L[:, :4]:\", L[:, :4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跳脫你的慣性思維，x 軸並不一定得水平展開。只要你想，這世上的任何直線都能是你的 x 軸。任何向量都可以是你描述手中數據的新基準。PCA 是一種拆解並重新表述數據的技巧，只要你想這世上的任何直線都能是你的 x 軸。任何向量都可以是你描述手中數據的新基準。PCA 就是一種拆解並重新表述數據的技巧，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "random_state = 9527 # 最大化 reproductivity\n",
    "\n",
    "pca_1d = PCA(1, random_state=random_state)\n",
    "L_sk = pca_1d.fit_transform(X.T).T \n",
    "\n",
    "print('L_sk.shape:', L_sk.shape)\n",
    "print('L_sk:', L_sk[:, :4])\n",
    "\n",
    "# sklearn API 得到的結果跟我們手動計算結果相同\n",
    "assert_almost_equal(L_sk, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "多數 Python 機器學習函式庫的預期輸入都是 n_samples 優先。\n",
    "這是為何在呼叫 scikit-learn 時我先轉置 X 使其維度變為 (n_samples, n_features)，\n",
    "接著再將其結果轉置回我想要的 (n_transformed_features, n_samples)：\n",
    "'''\n",
    "L_sk = pca_1d.fit_transform(X.T).T\n",
    "data = X.T\n",
    "L_transpose = pca_1d.transform(data)\n",
    "assert_almost_equal(L.T, L_transpose)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>expenses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>female</td>\n",
       "      <td>27.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>male</td>\n",
       "      <td>33.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.0</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.0</td>\n",
       "      <td>male</td>\n",
       "      <td>28.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age     sex   bmi  children smoker     region  expenses\n",
       "0  19.0  female  27.9       0.0    yes  southwest  16884.92\n",
       "1  18.0    male  33.8       1.0     no  southeast   1725.55\n",
       "2  28.0    male  33.0       3.0     no  southeast   4449.46\n",
       "3  33.0    male  22.7       0.0     no  northwest  21984.47\n",
       "4  32.0    male  28.9       0.0     no  northwest   3866.86"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_excel(\"input/insurance.xlsx\")\n",
    "data.head() #data.shape #data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<======== sex ===========>\n",
      "male      676\n",
      "female    662\n",
      "Name: sex, dtype: int64\n",
      "<======== smoker ===========>\n",
      "no     1064\n",
      "yes     274\n",
      "Name: smoker, dtype: int64\n",
      "<======== region ===========>\n",
      "southeast    364\n",
      "southwest    325\n",
      "northwest    325\n",
      "northeast    324\n",
      "Name: region, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Label Encode Object Types\n",
    "d_types = dict(data.dtypes)\n",
    "for name , type_ in d_types.items():\n",
    "    if str(type_) == 'object':\n",
    "        print(f\"<======== {name} ===========>\")\n",
    "        print(data[name].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for name , type_ in d_types.items():\n",
    "    if str(type_) == 'object':\n",
    "        Le = LabelEncoder()\n",
    "        data[name] = Le.fit_transform(data[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>expenses</th>\n",
       "      <th>region_2</th>\n",
       "      <th>region_3</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16884.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1725.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4449.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>21984.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3866.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex   bmi  children  smoker  expenses  region_2  region_3  region_1  \\\n",
       "0  19.0    0  27.9       0.0       1  16884.92       0.0       0.0       0.0   \n",
       "1  18.0    1  33.8       1.0       0   1725.55       0.0       0.0       1.0   \n",
       "2  28.0    1  33.0       3.0       0   4449.46       0.0       0.0       1.0   \n",
       "3  33.0    1  22.7       0.0       0  21984.47       0.0       1.0       0.0   \n",
       "4  32.0    1  28.9       0.0       0   3866.86       0.0       1.0       0.0   \n",
       "\n",
       "   region_0  \n",
       "0       1.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder()\n",
    "part = onehotencoder.fit_transform(data['region'].values.reshape(-1,1)).toarray()\n",
    "values = dict(data[\"region\"].value_counts())\n",
    "\n",
    "for e , (val , _) in enumerate(values.items()):\n",
    "    data[\"region_\" + str(val)] = part[:,e]\n",
    "\n",
    "data = data.drop([\"region\"] , axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_columns = list(data.columns)\n",
    "remaining_columns.remove(\"expenses\")\n",
    "\n",
    "X = data[remaining_columns].values \n",
    "Y = data['expenses'].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "Xtrain , Xtest , Ytrain , Ytest = train_test_split(X , Y , test_size = 0.2 , random_state = 4)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Scaler = StandardScaler()\n",
    "Xtrain = Scaler.fit_transform(Xtrain)\n",
    "Xtest = Scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardized (mean should be 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8be8045970>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "means = []\n",
    "\n",
    "plt.ylim(-1,1)\n",
    "for i in range(X.shape[1]):\n",
    "    means.append(np.mean(Xtest[:,i]))\n",
    "    \n",
    "plt.plot(means , scaley=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8c0834a910>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vars = []\n",
    "\n",
    "plt.ylim(0,2)\n",
    "for i in range(X.shape[1]):\n",
    "    vars.append(np.var(Xtest[:,i]))\n",
    "    \n",
    "plt.plot(vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LinearRegression\n",
    "### 普通線性回歸\n",
    "\n",
    "-  model.coef_ # 可查看係數\n",
    "-  model.intercept_ # 可查看截距值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(Xtrain , Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3845.2528539487685"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y = W.X + c\n",
    "model.coef_.dot(Xtest[10,:]) + model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3845.25285395])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(Xtest[10,:].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfecv.support_  #保留排名\n",
    "# rfecv.ranking_  #重要度排名\n",
    "# model.coef_ # 可查看係數\n",
    "# model.intercept_ # 可查看截距值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFECV) \n",
    "### 遞歸特徵消除：特徵擷取\n",
    "\n",
    "通過交叉驗證來找到最優的特徵數量。如果減少特徵會造成性能損失，那麼將不會去除任何特徵。這個方法用以選取單模型特徵相當不錯，但是有兩個缺陷，一，計算量大。二，隨著學習器（評估器）的改變，最佳特徵組合也會改變，有些時候會造成不利影響。\n",
    "\n",
    "- 對要訓練的機器學習算法進行建模\n",
    "- 確定在一次迭代中要消除的特徵數量。\n",
    "- 由於 RFECV 是遞歸迭代的，我們需要強行停止它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "#特徵選取\n",
    "model = LinearRegression()\n",
    "rfecv = RFECV(model , step = 1, min_features_to_select = 4 , n_jobs = -1) #每次去除特徵數  #最小特徵數\n",
    "rfecv.fit(Xtrain , Ytrain)\n",
    "model.fit(Xtrain , Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.88398812,  1.07913383, -0.90431146, ..., -0.56799694,\n",
       "        -0.62274236,  1.77860173],\n",
       "       [ 0.88398812, -0.09652155, -0.08913601, ..., -0.56799694,\n",
       "        -0.62274236,  1.77860173],\n",
       "       [-1.44174174,  0.58927742, -0.90431146, ..., -0.56799694,\n",
       "        -0.62274236,  1.77860173],\n",
       "       ...,\n",
       "       [-0.9484051 , -0.21082138, -0.90431146, ..., -0.56799694,\n",
       "        -0.62274236, -0.56223942],\n",
       "       [-1.08935843,  0.4096634 , -0.90431146, ...,  1.76057288,\n",
       "        -0.62274236, -0.56223942],\n",
       "       [ 1.44780141,  0.32802067, -0.90431146, ..., -0.56799694,\n",
       "        -0.62274236,  1.77860173]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features = np.where(rfecv.support_)[0]\n",
    "selected_features\n",
    "\n",
    "Xtrain = Xtrain[:,selected_features]\n",
    "Xtrain\n",
    "# Xtest = Xtest[:,selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3845.2528539487685"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y = W.X + c\n",
    "model.coef_.dot(Xtest[10,:]) + model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3845.25285395])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(Xtest[10,:].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
