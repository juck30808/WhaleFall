{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Employee_Data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CTI110~1\\AppData\\Local\\Temp/ipykernel_28220/2915000410.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Employee_Data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'max_columns'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m35\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Employee_Data.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "data = pd.read_csv('Employee_Data.csv')\n",
    "pd.set_option('max_columns', 35)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the head of the dataset\n",
    "pd.set_option('max_columns', 35)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets Calculate the Average Time Span of Employees in a Company\n",
    "data['AvgTimeSpanAtCompanies'] = (data['TotalWorkingYears'])/(data['NumCompaniesWorked'] + 1)\n",
    "\n",
    "# Lets check the Values\n",
    "sns.distplot(data['AvgTimeSpanAtCompanies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let Create a New Column called Overall Satisfaction\n",
    "data['OverallSatisfaction'] = (data['EnvironmentSatisfaction'] + data['RelationshipSatisfaction'] + data['JobSatisfaction'])\n",
    "\n",
    "# lets check the Values\n",
    "sns.distplot(data['OverallSatisfaction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets Bin the Age into Categories\n",
    "\n",
    "# lets create a Function for Categorization of age\n",
    "def age_cat(x):\n",
    "    if x < 40:\n",
    "        return 'Young'\n",
    "    else:\n",
    "        return 'Old'\n",
    "    \n",
    "# lets apply the function created\n",
    "data['Age'] = data['Age'].apply(age_cat)\n",
    "\n",
    "# lets check the Values After Binning the Values\n",
    "data['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets Bin the Distance from Home also\n",
    "\n",
    "# lets Create a Function for Categoriation of Distance from Home\n",
    "def dist_cat(x):\n",
    "    if x <= 2:\n",
    "        return \"Near\"\n",
    "    elif x > 2 and x < 5:\n",
    "        return \"Far\"\n",
    "    else:\n",
    "        return \"Very Far\"\n",
    "        \n",
    "# lets apply the Function\n",
    "data['DistanceFromHome'] = data['DistanceFromHome'].apply(dist_cat)\n",
    "\n",
    "# lets check the values\n",
    "data['DistanceFromHome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets Bin the Total Working Years also\n",
    "\n",
    "# lets Create a Function for Categoriation of Distance from Home\n",
    "def work_cat(x):\n",
    "    if x <= 2:\n",
    "        return \"Fresher\"\n",
    "    else:\n",
    "        return \"Professional\"\n",
    "        \n",
    "# lets apply the Function\n",
    "data['TotalWorkingYears'] = data['TotalWorkingYears'].apply(work_cat)\n",
    "\n",
    "# lets check the values\n",
    "data['TotalWorkingYears'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets find out all the Useless Columns\n",
    "\n",
    "print(\"Number of Records in the Dataset :\", data.shape[0])\n",
    "print(\"Number of Unique Values in Employee Count Column :\", data['EmployeeCount'].nunique())\n",
    "print(\"Number of Unique Values in Employee Number Column :\", data['EmployeeNumber'].nunique())\n",
    "print(\"Number of Unique Values in Over 18 Column :\", data['Over18'].nunique())\n",
    "print(\"Number of Unique Values in Standard Hours Column :\", data['StandardHours'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see that either these columns has all the values unique such as the Employee Number Column or these Columns has a Unique Value.\n",
    "* These Kinds of Columns are useless for Predictive analysis as they do not have any Trends or Patterns Associated with them.\n",
    "* Lets Drop these Columns from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets remove these Columns from the Dataset\n",
    "\n",
    "data = data.drop(['EmployeeCount', 'EmployeeNumber',\n",
    "                 'Over18', 'StandardHours'], axis = 1)\n",
    "\n",
    "# lets check the Number of Columns after Removal of 4 columns from 35 Columns\n",
    "print(\"Number of Columns Left in the Dataset after Removal of 4 Columns :\", data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check Missing Values\n",
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the Columns with Object Data Types\n",
    "data.select_dtypes('object').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Encode these Object Data Types as Numerical Data Types\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['BusinessTravel'] = le.fit_transform(data['BusinessTravel'])\n",
    "data['Department'] = le.fit_transform(data['Department'])\n",
    "data['EducationField'] = le.fit_transform(data['EducationField'])\n",
    "data['Gender'] = le.fit_transform(data['Gender'])\n",
    "data['JobRole'] = le.fit_transform(data['JobRole'])\n",
    "data['MaritalStatus'] = le.fit_transform(data['MaritalStatus'])\n",
    "data['OverTime'] = le.fit_transform(data['OverTime'])\n",
    "\n",
    "# lets again check whether there any Object Data Type Columns are Left\n",
    "data.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets split the Target Column Attrition from the Dataset\n",
    "\n",
    "y = data['Attrition']\n",
    "x = data.drop(['Attrition'], axis = 1)\n",
    "\n",
    "# lets check the shape of the dataset\n",
    "print(\"Shape of x: \", x.shape)\n",
    "print(\"Shape of y: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Check the Attrition Column\n",
    "data['Attrition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.combine import SMOTEENN\n",
    "\n",
    "# # defining the Model\n",
    "# sn = SMOTEENN(random_state = 0)\n",
    "\n",
    "# # Training the Model\n",
    "# sn.fit(x, y)\n",
    "\n",
    "# # Making the Samples\n",
    "# x, y = sn.fit_sample(x, y)\n",
    "\n",
    "# # Class Distribution\n",
    "# y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"Shape of xtrain: \", x_train.shape)\n",
    "print(\"Shape of xtest: \", x_test.shape)\n",
    "print(\"Shape of ytrain :\", y_train.shape)\n",
    "print(\"Shape of ytest :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets create a Predictive Model\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# model = DecisionTreeClassifier()\n",
    "# model.fit(x_train, y_train)\n",
    "# y_pred = model.predict(x_test)\n",
    "\n",
    "# cr = classification_report(y_pred, y_test)\n",
    "# print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Filtering\n",
    "\n",
    "* Remove Highly Correlated Features from the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "corr = data.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype = bool))\n",
    "sns.heatmap(corr, mask = mask, cmap = 'Wistia', linewidths = 0.2)\n",
    "plt.xticks(fontsize = 10)\n",
    "plt.yticks(fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Age is Highly Correlated with Job Level, Monthly Income, and TotalWorking Years \n",
    "* Job Role and Department are Highly Correlated\n",
    "* Job Level is Highly Correlated with TotalWorkingyears and MonthlyIncome\n",
    "* Performance Rating and PercentSalaryHike are Highly Correlated\n",
    "* YearsInCurrentRole is Highly Correlated with YearsAtCompany and YearsWithCurrManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Remove the Highly Correlated Columns from the Datasets\n",
    "\n",
    "data_cf = data.drop(['JobRole', 'JobLevel', 'PercentSalaryHike', 'TotalWorkingYears',\n",
    "                     'YearsInCurrentRole', 'YearsWithCurrManager'], axis = 1)\n",
    "\n",
    "# lets check the shape of the dataset after removal of 5 Columns from 35 Columns\n",
    "data_cf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Encode these Object Data Types as Numerical Data Types\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data_cf['BusinessTravel'] = le.fit_transform(data_cf['BusinessTravel'])\n",
    "data_cf['Department'] = le.fit_transform(data_cf['Department'])\n",
    "data_cf['EducationField'] = le.fit_transform(data_cf['EducationField'])\n",
    "data_cf['Gender'] = le.fit_transform(data_cf['Gender'])\n",
    "data_cf['MaritalStatus'] = le.fit_transform(data_cf['MaritalStatus'])\n",
    "data_cf['OverTime'] = le.fit_transform(data_cf['OverTime'])\n",
    "\n",
    "# lets again check whether there any Object Data Type Columns are Left\n",
    "data_cf.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, Split the Dataset\n",
    "\n",
    "y_cf = data_cf['Attrition']\n",
    "x_cf = data_cf.drop(['Attrition'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.combine import SMOTEENN\n",
    "\n",
    "# # defining the Model\n",
    "# sn = SMOTEENN(random_state = 0)\n",
    "\n",
    "# # Training the Model\n",
    "# sn.fit(x_cf, y_cf)\n",
    "\n",
    "# # Making the Samples\n",
    "# x_cf, y_cf = sn.fit_sample(x_cf, y_cf)\n",
    "\n",
    "# # Class Distribution\n",
    "# y_cf.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_cf, x_test_cf, y_train_cf, y_test_cf = train_test_split(x_cf, y_cf, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"Shape of xtrain: \", x_train_cf.shape)\n",
    "print(\"Shape of xtest: \", x_test_cf.shape)\n",
    "print(\"Shape of ytrain :\", y_train_cf.shape)\n",
    "print(\"Shape of ytest :\", y_test_cf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a Predictive Model\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train_cf, y_train_cf)\n",
    "y_pred_cf = model.predict(x_test_cf)\n",
    "\n",
    "cr = classification_report(y_pred_cf, y_test_cf)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Inflation Filtering\n",
    "\n",
    "* Remove all the Features having High Variance\n",
    "* VIF above 5 indicates a high multicollinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "\n",
    "# # VIF dataframe \n",
    "# vif_data = pd.DataFrame() \n",
    "# vif_data[\"feature\"] = x.columns \n",
    "  \n",
    "# # calculating VIF for each feature \n",
    "# vif_data[\"VIF\"] = [variance_inflation_factor(x.values, i) \n",
    "#                           for i in range(len(x.columns))] \n",
    "  \n",
    "# vif_data.style.background_gradient(cmap = 'Wistia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create the Dependent and Non Dependent Sets\n",
    "x_vif = data[['DistanceFromHome', 'EducationField', 'Gender', 'NumCompaniesWorked','OverTime',\n",
    "           'StockOptionLevel','YearsSinceLastPromotion']]\n",
    "y_vif = data['Attrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.combine import SMOTEENN\n",
    "\n",
    "# # defining the Model\n",
    "# sn = SMOTEENN(random_state = 0)\n",
    "\n",
    "# # Training the Model\n",
    "# sn.fit(x_vif, y_vif)\n",
    "\n",
    "# # Making the Samples\n",
    "# x_vif, y_vif = sn.fit_sample(x_vif, y_vif)\n",
    "\n",
    "# # Class Distribution\n",
    "# y_vif.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_vif, x_test_vif, y_train_vif, y_test_vif = train_test_split(x_vif, y_vif, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"Shape of xtrain: \", x_train_vif.shape)\n",
    "print(\"Shape of xtest: \", x_test_vif.shape)\n",
    "print(\"Shape of ytrain :\", y_train_vif.shape)\n",
    "print(\"Shape of ytest :\", y_test_vif.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets create a Predictive Model\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# model = DecisionTreeClassifier()\n",
    "# model.fit(x_train_vif, y_train_vif)\n",
    "# y_pred_vif = model.predict(x_test_vif)\n",
    "\n",
    "# cr = classification_report(y_pred_vif, y_test_vif)\n",
    "# print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection with RFECV\n",
    "\n",
    "* Recursive Elimination of Features from the Dataset to Reduce the Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "model = DecisionTreeClassifier() \n",
    "rfecv = RFECV(estimator = model, step = 1, cv = 5, scoring = 'accuracy')\n",
    "rfecv = rfecv.fit(x_train, y_train)\n",
    "\n",
    "print('Optimal number of features :', rfecv.n_features_)\n",
    "print('Best features :', x_train.columns[rfecv.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets Create the Dependent and Independent Sets for RFECV\n",
    "\n",
    "x_rfecv = data[['JobLevel','MonthlyIncome','MonthlyRate']]\n",
    "y_rfecv = data['Attrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# defining the Model\n",
    "sn = SMOTEENN(random_state = 0)\n",
    "\n",
    "# Training the Model\n",
    "sn.fit(x_rfecv, y_rfecv)\n",
    "\n",
    "# Making the Samples\n",
    "x_rfecv, y_rfecv = sn.fit_sample(x_rfecv, y_rfecv)\n",
    "\n",
    "# Class Distribution\n",
    "y_rfecv.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_rfecv, x_test_rfecv, y_train_rfecv, y_test_rfecv = train_test_split(x_rfecv, y_rfecv,\n",
    "                                                                            test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"Shape of xtrain: \", x_train_rfecv.shape)\n",
    "print(\"Shape of xtest: \", x_test_rfecv.shape)\n",
    "print(\"Shape of ytrain :\", y_train_rfecv.shape)\n",
    "print(\"Shape of ytest :\", y_test_rfecv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a Predictive Model\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train_rfecv, y_train_rfecv)\n",
    "y_pred_rfecv = model.predict(x_test_rfecv)\n",
    "\n",
    "cr = classification_report(y_pred_rfecv, y_test_rfecv)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection using the Boruta Algorithm\n",
    "\n",
    "* In Boruta, features do not compete among themselves. Instead — and this is the first brilliant idea — they compete with a randomized version of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Boruta Algorithm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "\n",
    "# defining the Model\n",
    "model= RandomForestClassifier(max_depth = 5)\n",
    "\n",
    "# define Boruta feature selection method\n",
    "boruta = BorutaPy(estimator = model, n_estimators = 'auto',\n",
    "                  max_iter = 100, random_state = 0)\n",
    "\n",
    "# find all relevant features\n",
    "boruta.fit(np.array(x), np.array(y))\n",
    "\n",
    "### print results\n",
    "best_features = x.columns[boruta.support_].to_list()\n",
    "print('features to Keep:', best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets Create the Dependent and Independent Sets for RFECV\n",
    "\n",
    "x_bor = data[['Age', 'DailyRate', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction',\n",
    "                'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction',\n",
    "                'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'RelationshipSatisfaction',\n",
    "                'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance',\n",
    "                'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']]\n",
    "y_bor = data['Attrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# defining the Model\n",
    "sn = SMOTEENN(random_state = 0)\n",
    "\n",
    "# Training the Model\n",
    "sn.fit(x_bor, y_bor)\n",
    "\n",
    "# Making the Samples\n",
    "x_bor, y_bor = sn.fit_sample(x_bor, y_bor)\n",
    "\n",
    "# Class Distribution\n",
    "y_bor.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_bor, x_test_bor, y_train_bor, y_test_bor = train_test_split(x_bor, y_bor,\n",
    "                                                                            test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"Shape of xtrain: \", x_train_bor.shape)\n",
    "print(\"Shape of xtest: \", x_test_bor.shape)\n",
    "print(\"Shape of ytrain :\", y_train_bor.shape)\n",
    "print(\"Shape of ytest :\", y_test_bor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a Predictive Model\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train_bor, y_train_bor)\n",
    "y_pred_bor = model.predict(x_test_bor)\n",
    "\n",
    "cr = classification_report(y_pred_bor, y_test_bor)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "* The principal components of a collection of points in a real p-space that are a sequence of p direction vectors, where the ith vector is the direction of a line that best fits the data while being orthogonal to the first i-1 vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets determine the Value of N-Components\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Lets create the PCA Model\n",
    "pca = PCA(n_components = None)\n",
    "x_train_pca = pca.fit_transform(x_train)\n",
    "x_test_pca = pca.transform(x_test)\n",
    "\n",
    "# This result the variance explained by the number of components taken into account\n",
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Lets create the PCA Model\n",
    "pca = PCA(n_components = 2)\n",
    "x_train_pca = pca.fit_transform(x_train)\n",
    "x_test_pca = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a Predictive Model\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train_pca, y_train)\n",
    "y_pred_pca = model.predict(x_test_pca)\n",
    "\n",
    "cr = classification_report(y_pred_pca, y_test)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check How these Components will look\n",
    "\n",
    "df = pd.DataFrame({'x_train_pca0':x_train_pca[:,0],\n",
    "                 'x_train_pca1':x_train_pca[:,1],\n",
    "                 'Attrition':y_train})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot the Principal Components\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (9, 4)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "sns.scatterplot(df['x_train_pca0'], df['x_train_pca1'],\n",
    "                hue = df['Attrition'])\n",
    "plt.title('Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "* t-distributed stochastic neighbor embedding is a machine learning algorithm for visualization based on Stochastic Neighbor Embedding originally developed by Sam Roweis and Geoffrey Hinton, where Laurens van der Maaten proposed the t-distributed variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components = 2)\n",
    "x_train_tsne = pca.fit_transform(x_train)\n",
    "x_test_tsne = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a Predictive Model\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train_tsne, y_train)\n",
    "y_pred_tsne = model.predict(x_test_tsne)\n",
    "\n",
    "cr = classification_report(y_pred_tsne, y_test)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis\n",
    "* a technique that is used to reduce a large number of variables into fewer numbers of factors. The values of observed data are expressed as functions of a number of possible causes in order to find which are the most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# lets define the Model\n",
    "lda = LDA(n_components = None)\n",
    "x_lda = lda.fit_transform(x_train, y_train)\n",
    "\n",
    "# Create array of explained variance ratios\n",
    "lda_var_ratios = lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function\n",
    "def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "    # Set initial variance explained so far\n",
    "    total_variance = 0.0\n",
    "    \n",
    "    # Set initial number of features\n",
    "    n_components = 0\n",
    "    \n",
    "    # For the explained variance of each feature:\n",
    "    for explained_variance in var_ratio:\n",
    "        \n",
    "        # Add the explained variance to the total\n",
    "        total_variance += explained_variance\n",
    "        \n",
    "        # Add one to the number of components\n",
    "        n_components += 1\n",
    "        \n",
    "        # If we reach our goal level of explained variance\n",
    "        if total_variance >= goal_var:\n",
    "            # End the loop\n",
    "            break\n",
    "            \n",
    "    # Return the number of components\n",
    "    return n_components\n",
    "\n",
    "# lets run the function\n",
    "select_n_components(lda_var_ratios, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define the Model\n",
    "lda = LDA(n_components = 1)\n",
    "x_train_lda = lda.fit_transform(x_train, y_train)\n",
    "x_test_lda = lda.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a Predictive Model\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train_lda, y_train)\n",
    "y_pred_lda = model.predict(x_test_lda)\n",
    "\n",
    "cr = classification_report(y_pred_lda, y_test)\n",
    "print(cr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
